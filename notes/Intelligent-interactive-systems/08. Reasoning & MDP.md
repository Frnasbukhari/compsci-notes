# Probabilistic Reasoning & Markov Decision Processes  


## 1. Why This Matters  
Modern AI systems—and the humans who use them—must make good choices in the face of uncertainty. Two fundamental toolkits help:  

1. **Bayesian reasoning** — using evidence to update beliefs about how likely something is.  
2. **Markov Decision Processes (MDPs)** — formal road-maps for picking actions over time when results depend on both choice *and* chance.

---

## 2. Bayes’ Theorem (Bayes’ Law) — *Updating beliefs from data*  
*Technical term*: **Bayes’ theorem** — a formula that flips conditional probabilities so we can answer “Given a positive test, how likely is the disease?”  

> **Formula**:  
> \$P(H\mid E)=\\dfrac{P(E\mid H)\,P(H)}{P(E)}\$  
> *Plain English*: Chance of the **hypothesis** \$H\$ after seeing **evidence** \$E\$ equals how often we’d see the evidence if the hypothesis were true, times how common the hypothesis is overall, divided by how often we see the evidence in general.

### 2.1 Natural-Frequency Trick  
Present numbers as simple counts (“9 out of 10”) instead of tricky percentages. Humans then answer Bayes problems far better. 

| **Statistic Style** | **Example for Disease Q** | **Cognitive Load** |
|---------------------|---------------------------|--------------------|
| Conditional %       | “Sensitivity = 90 %, False-positive = 9 %” | High |
| Natural frequency   | “Out of 1,000 people, 10 have Q; 9 of them test positive; 89 healthy people also test positive.” | Low |

---

## 3. Markov Chains — *Probabilistic dominoes*  
*Technical term*: **Markov chain** — a sequence where tomorrow’s state depends only on today, not yesterday. Think weather: if it’s rainy today there’s a fixed chance it stays rainy tomorrow. 

---

## 4. Markov Decision Process (MDP) — *A Markov chain with choices*  
*Technical term*: **MDP** — a Markov chain plus *actions* and *rewards* so an “agent” can seek long-term gain. Plainly: a game board where each move changes both your position and your score.

### 4.1 Anatomy of an MDP  

| Symbol | Name | Plain-English meaning |
|--------|------|-----------------------|
| \$S\$ | **States** | Situations you can be in |
| \$A\$ | **Actions** | Moves you may take |
| \$P(s' \| s,a)\$ | **Transition function** | Dice that say where you’ll land after action \$a\$ in state \$s\$ |
| \$R(s',s)\$ or \$R(s,a)\$ | **Reward** | Instant payoff (can be $ , QALY, points, etc.) |

### 4.2 Bellman Equation — *Value in two parts*  
**Bellman value** \$V^\* (s)\$ — best long-run reward you can still earn from state \$s\$:     

$$V^\*(s)=\max_{a\in A}\Big( R(s,a)+\gamma\sum_{s'}P(s'\mid s,a)\,V^\*(s') \Big)$$      

Plain English: Pick the action whose *immediate* reward plus the *discounted* future goodies is largest. 

---

## 5. Solving an MDP — *Turning math into policy*  

1. **Value Iteration** — repeatedly plug Bellman back into itself until values stop changing.  
2. **Policy Iteration** — guess a policy, evaluate it, then improve it and repeat.  
3. **Q-Learning & RL** — learn good actions from experience instead of knowing \$P\$ beforehand.  
4. **Deep RL / PPO** — add neural nets to scale learning to huge state spaces.

---

## 6. Healthcare Case Studies — *MDPs in the clinic*  

### 6.1 Treatment Paths for Chronic Disease  
Researchers built MDPs to choose *when* to start **statin therapy**, accept a **liver donation**, or adjust **diabetes meds** so lifetime quality (measured in *Quality-Adjusted Life Years*, QALY) is maximized.

### 6.2 Key Trade-offs  
- **Immediate relief** vs **long-term survival**  
- **Medication side-effects** vs **disease progression**  
MDP models surfaced counter-intuitive optimal choices—e.g., delaying a transplant until health slightly worsens increases expected life-years overall.

---

## 7. Situation Awareness (SA) in Aviation — *Eyes as actions*  

Air-crew must scan dozens of instruments. Modeling each eye fixation as an **action** and SA as **state**, an MDP predicts optimal scan sequences that match expert pilots. Future uses: pilot-training feedback, dashboard layout, even autonomous-car driver monitoring. 

---

## 8. AI Framework for Clinical Decision-Making — *MDP meets POMDP*  

**Partially Observable MDP (POMDP)** — like an MDP but you can’t see the true state; you hold a **belief state** (probability distribution) instead.  
A 2013 framework embedded MDP/POMDP logic into a **multi-agent system** that simulates physicians and patients, optimizing a utility called **Cost Per Unit Change (CPUC)**: money spent to improve one unit of health. Results beat treatment-as-usual by cutting CPUC from \$497 to \$189. 

---

## 9. Statistical Literacy — *Necessary human upgrade*  

Many doctors and patients misread risks. Remedies:  

| **Bad Practice** | **Better Practice (why)** |
|------------------|---------------------------|
| Relative risk (“cuts risk by 50 %”) | Absolute risk (“from 2 % to 1 %”) — reveals true size |
| 5-year survival rates | Mortality rates — avoids lead-time bias |
| Conditional probabilities | Natural frequencies — easier mental math |

Teaching these skills empowers genuine **shared decision-making** instead of blind trust.

---

## 10. Take-Home Messages  

1. **Translate math into counts** — natural frequencies clarify Bayesian problems.  
2. **Model sequential choices** — MDPs turn fuzzy “what ifs” into solvable equations.  
3. **Plan for the long term** — Bellman rewards foresight.  
4. **Algorithms abound** — value iteration to deep RL scales solutions.  
5. **Real impact** — medical policies, pilot vigilance, and AI doctors already use these ideas.  
6. **Human factors matter** — clear statistics keep people in the loop and decisions ethical.
