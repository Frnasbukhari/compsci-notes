# Recommender Systems
A **Recommender System (RS)** —*a program that filters information for a user based on what it knows about that user*— predicts what each person will probably like and presents those items.

## 1.  Motivation: From Choice Overload to Personalization
<img src="https://pub.mdpi-res.com/applsci/applsci-10-05510/article_deploy/html/images/applsci-10-05510-g001.png?1597026757" alt="RS" width="400">

Modern platforms present users with *millions* of options; surfacing the right few items is the
difference between delight and churn.

### 1.1 The Problem Space
- **Information Overload** – Users cannot manually scan vast catalogs.
- **Attention Economy** – Competing apps are one swipe away.
- **Cognitive Effort** – People abandon choice tasks that feel hard.

### 1.2 Why Businesses Care
| Platform | % activity attributed to RS | Reference Impact |
|----------|----------------------------|------------------|
| Amazon   | ~35 % of sales             | Revenue uplift & cart size |
| Netflix  | ~75 % of streams           | Retention & watch time      |
| YouTube  | ~70 % of watch time        | Ad inventory & engagement   |

*Take‑home message ➜* Personalization is *not* cosmetic; it is **mission‑critical** for both user experience and bottom‑line metrics.

---

## 2.  Core Building Blocks & Terminology
With the *why* clarified, we now need a shared vocabulary before digging into algorithms.

| Term | Plain‑English Meaning | Typical Examples |
|------|----------------------|------------------|
| **User Model** | Everything the system knows about a person | ratings, clicks, age |
| **Item** | The thing being recommended | movie, news article |
| **Feedback** | Signal that reveals preference | 5‑star rating, dwell time |
| **Utility Function** | Math rule scoring *user × item* pairs | predicted rating |
| **Long Tail** | Large set of niche items | indie films, rare books |


---

## 3.  Taxonomy of Recommender Approaches
Armed with terminology, we can map the solution landscape.  
Every production system is some flavor—or mixture—of the following four paradigms.

| Approach | Core Idea | Strength | Weakness |
|----------|-----------|----------|----------|
| **Collaborative Filtering (CF)** | *Users who agreed in the past will agree in the future.* | Captures latent taste signals | Cold‑start & popularity bias |
| **Content‑Based (CB)** | *Recommend items similar to those the user liked.* | Works with small user base | Over‑specialization |
| **Hybrid** | *Blend CF + CB scores or models.* | Balances weaknesses | Added complexity |
| **Knowledge‑Based (KB)** | *Use explicit domain rules/constraints.* | Handles sparse data | Heavy manual curation |

---

## 4.  Collaborative Filtering
<img src="https://miro.medium.com/v2/resize:fit:1358/0*xA2P_g7T4TOPRKJE.png" alt="CF" width="400">

CF is historically the workhorse of RS; many “People also like…” widgets you see are CF under the hood.

### 4.1 Mechanics
CF builds a **user–item matrix**  
( rows = users, columns = items, cells = ratings or implicit counts).

#### Two Schools
| Flavor | What It Does | Typical Similarity Metric |
|--------|--------------|---------------------------|
| **User‑Based** | Finds users most similar to *you* and averages their ratings | Pearson correlation |
| **Item‑Based** | Finds items similar to what you liked | Cosine similarity or adjusted cosine |

> **When to pick which?**  
> • User‑based is intuitive but struggles at scale (>10 M users).  
> • Item‑based pre‑computes similarity offline, scaling better with large catalogs.

#### Ratings  
- **Explicit Ratings** —*stars, thumbs-up*: precise but hard to collect.  
- **Implicit Ratings** —*clicks, purchases*: easy but noisy.

### 4.2 Dealing with Implicit Data
- Binary events (clicked / not clicked) → treat as *positive‑only* matrix.  
- Weight by confidence: $c_{ui} = 1 + \alpha \log(1 + n_{ui})$.

### 4.3 Pain Points & Mitigations
| Issue | Symptom | Popular Fix |
|-------|---------|-------------|
| **Cold Start** | New user gets generic feed | ask onboarding quiz, leverage CB |
| **Data Sparsity** | 99 % of matrix empty | matrix factorization, autoencoders |
| **Scalability** | O(U × I) naive cost | MinHash LSH, model quantization |
| **Bias** | Recommends only mainstream | calibrated ranking, diversity regularizers |

---

## 5.  Content‑Based Recommendation
<img src="https://i0.wp.com/spotintelligence.com/wp-content/uploads/2023/11/content-based-recommendation-system.jpg?fit=1200%2C675&ssl=1" alt="CB" width="400">

CF relies on *others*; CB relies on the *item itself*, making it a perfect complement.

### 5.1 Representing Content
1. **Textual Items** → TF‑IDF vectors, word embeddings, or transformer sentence embeddings.  
2. **Images** → CNN feature vectors (e.g., ResNet penultimate layer).  
3. **Audio/Video** → Spectrogram embeddings, 3D CNNs.  

> **Rule of Thumb:** Choose the *simplest* representation that captures discriminative signal.

### 5.2 Learning the User Profile
- **Vector aggregation**: average vectors of liked items to form user centroid.  
- **Supervised models**: treat rating prediction as classification/regression (SVM, XGBoost).  
- **Deep learning**: sequence models that capture temporal taste drift.

### 5.3 Addressing Limitations
| Limitation | Why It Happens | Design Countermeasure |
|------------|----------------|-----------------------|
| Overspecialization | cosine similarity favors similar items | inject serendipity via ε‑greedy |
| Feature sparseness | short titles, no rich metadata | auto‑tagging models, multimodal fusion |
| Cold user start | no liked items yet | short preference quiz, demographic priors |

---

## 6.  Hybrid & Knowledge‑Based Systems
When you *blend* CF and CB, you often get the “80 / 20 win” in practice.

### 6.1 How to Hybridize
1. **Score‑Level Fusion** – weighted sum of CF & CB scores.  
2. **Model Stacking** – feed CF & CB outputs into a meta‑learner.  
3. **Switching Hybrid** – choose CB for new users, CF otherwise.  
4. **Feature Augmentation** – use CB features as input to CF factorization.

### 6.2 Knowledge‑Based RS
- Encode constraints (e.g., *camera lens must fit Nikon F mount*).  
- Use **Case‑Based Reasoning** for high‑involvement purchases (cars, apartments).

---

## 7.  Evaluating Success

### 7.1 Offline Metrics
| Objective | Metric | Good When |
|-----------|--------|-----------|
| Rating prediction | RMSE, MAE | dataset has dense explicit ratings |
| Top‑k ranking | Precision@k, Recall@k, NDCG | want user to find what they already like |
| Diversity | Intra‑list similarity, coverage | reduce filter bubble |

### 7.2 Online & Business Metrics
| Layer | KPI | Insight |
|-------|-----|---------|
| UX | Dwell time, session length | engagement |
| Conversion | CTR, add‑to‑cart rate | revenue |
| Long‑term | Retention, LTV | sustainability |

---

## 8.  Frontier Research & Industry Trends

1. **Contrastive Learning for Implicit Feedback**  
2. **Bandit & Reinforcement Learning Loops**  
3. **Explainable Recommendations** (e.g., “Because you watched *Inception*…”)  
4. **Fairness & Responsible AI** — audit exposure of minority‑owned content.  
5. **On‑device Personalization** — privacy‑preserving federated learning.  
6. **Multimodal & Cross‑domain Transfer** — leverage signal across media types.  
