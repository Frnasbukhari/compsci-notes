# Unsupervised Learning & Topic Modeling

## 1. Why Unsupervised?
When labels are scarce or subjective, **unsupervised learning** lets the **data distribution itself** reveal structure.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*zKHCMnHzlLGrxgAWY8RDtw.png" alt="clustering" width="400">


* **Hard clustering** (e.g., K‑means) – each sample belongs to exactly one cluster.  
* **Soft / model‑based clustering** (e.g., Gaussian Mixtures, LDA) – samples share fractional membership across latent groups.

---

## 2. Hard Clustering with K‑means
<img src="https://www.ejable.com/wp-content/uploads/2023/11/Stop-point-for-K-Means-clustering-2.webp" alt="K‑means" width="400">

### 2.1 Algorithm
| Step | Action |
|------|--------|
| 1️⃣ | **Initialise**: choose *K* random centroids. |
| 2️⃣ | **Assign**: allocate each point to its nearest centroid. |
| 3️⃣ | **Update**: move each centroid to the mean of its assigned points. |
| 4️⃣ | **Repeat** steps 2–3 until convergence (assignments stop or centroids stabilise). |

### 2.2 Math Corner
*Centroid*  

$$\mu(\omega) = \frac{1}{|\omega|} \sum_{x \in \omega} x$$

*Residual Sum of Squares*  

$$RSS = \sum_{k=1}^{K} \sum_{x \in \omega_k} \lVert x - \mu(\omega_k) \rVert^2$$

### 2.3 Choosing K
* **Elbow / knee on RSS(K)** – plot RSS vs *K*; pick the first sharp bend.  
* **Silhouette score**  

$$s = \frac{b - a}{\max(a, b)} \quad (-1 \le s \le 1)$$  

where *a* = average distance to points in the **same** cluster, *b* = average distance to the **nearest other** cluster.

---

## 3. External Cluster Evaluation

| Metric | Intuition | Formula (summary) |
|--------|-----------|-------------------|
| **Purity** | Majority label per cluster | Increases with many tiny clusters |
| **Homogeneity (h)** | “Each cluster contains one class” | $h = 1 - \tfrac{H(C\mid K)}{H(C)}$ |
| **Completeness (c)** | “Members of a class end up in the same cluster” | $c = 1 - \tfrac{H(K\mid C)}{H(K)}$ |
| **V‑measure** | Harmonic mean of *h* & *c* | $V_\beta = \tfrac{(1+\beta)hc}{\beta h + c}$ |

---

## 4. From Counts to Topics — Latent Dirichlet Allocation (LDA)
<img src="https://miro.medium.com/v2/resize:fit:850/1*VK5R_2YlRx3NbRIMbUxJ8Q.png" alt="LDA" width="400">

### 4.1 Why "Topics Analysis" at all?
Latent Dirichlet Allocation (**LDA — statistical recipe for finding hidden themes**) models documents as blends of underlying *topics* (word-distribution patterns) instead of raw word bags. This cuts noise, reveals structure, and feeds downstream tasks like classification.

### 4.2 From Raw Counts to Smarter Counts
1. **tf‑idf (term frequency–inverse document frequency — how often a word appears, discounted if it’s everywhere)** turns each document into a vector of weighted counts.  
2. **Singular Value Decomposition (SVD — matrix‑factoring shortcut that keeps only dominant patterns)** underlies **Latent Semantic Indexing (LSI — early “semantic space” trick)**, projecting the huge *document × word* grid into a compact latent space that loosely captures synonymy/polysemy.

### 4.3 Probabilistic Turn: Comparing Topic Models

| Model | Plain‑English Pitch | Advantages | Limitations |
|-------|--------------------|------------|-------------|
| **Mixture of Unigrams** | “1 topic per doc.” | Simple math | Unrealistic for multi‑theme docs |
| **pLSI (probabilistic LSI — pick a topic for every word)** | Allows word‑level topic switches | Captures mixed docs | No true generative doc model → can’t score new docs |
| **LDA (Latent Dirichlet Allocation)** | Full generative story for corpora | Handles unseen docs, Bayesian elegance | Needs approximate inference |


### 4.4 LDA’s Generative Story
1. Draw **topic mixture** for the document: $\theta \sim \text{Dir}(\alpha)$.  
2. For each word *n* (total *N*):  
   a. Pick topic $z_n \sim \text{Mult}(\theta)$.  
   b. Emit word $w_n \sim p(w\mid z_n, \beta)$.

The Dirichlet prior allows reuse of topics within a document and scoring of unseen docs.

### 4.5 Why LDA Beats Pure Clustering
* Soft, interpretable “topic–word” distributions.  
* Documents receive **fingerprints** (topic proportions) rather than hard labels.  
* Extensible (Correlated, Dynamic, Hierarchical LDA).

---

## 5. Inference & Hyperparameter Tuning

| Goal | Common Approach | Notes |
|------|-----------------|-------|
| Estimate hidden vars | **Variational EM** | Fast, deterministic, default in *gensim*. |
| Better accuracy | **Collapsed Gibbs sampling** | Slower but simple math. |
| Optimise \$alpha, \eta$ | **Empirical Bayes** | Re‑estimate every few EM iterations. |

---

## 6. Minimal Python Pipeline

```python
# 0. pip install gensim pyldavis
docs = [clean(t) for t in raw_texts]

# 1. Bag‑of‑Words prep
id2word = Dictionary(docs)
corpus = [id2word.doc2bow(d) for d in docs]

# 2. Search K via topic coherence
for k in range(5, 21):
    lda = LdaModel(corpus, id2word, num_topics=k, passes=10, alpha='auto')
    c_v = lda.top_topics(corpus=corpus, coherence='c_v')[0][1]
    print(k, lda.log_perplexity(corpus), c_v)
```

*Visualise topics:*  

```python
import pyLDAvis.gensim_models as gensimvis
pyldavis_html = gensimvis.prepare(lda, corpus, id2word)
```

---

## 7. Tool Selection Quick Guide

| Use‑case | Pick |
|----------|------|
| One dominant theme per item | **K‑means** (fast, scalable) |
| Mixed themes, dimensionality reduction, recommender latent factors | **LDA** |
| Non‑spherical clusters | EM‑Gaussian, Spectral, DBSCAN |

---

## 8. Common Pitfalls & Remedies

| Pitfall | Symptom | Remedy |
|---------|---------|--------|
| Too many topics | Redundant / incoherent topics | Tune *K* by coherence; merge similar topics. |
| Rare words dominate | Singleton, meaningless topics | Trim low & high‑frequency tokens; apply TF‑IDF. |
| Bag‑of‑Words limits | Missing multi‑word phrases | Add n‑grams or use phrase‑aware LDA. |
| Slow convergence | Training lags on large corpora | Use online/streaming LDA, adjust `chunksize`. |

---

## 9. Further evaluation metrics
| Metric | Measures | Watch‑Out |
| --- | --- | --- |
| **Purity** | % of docs whose majority class matches their cluster. | Inflated by many tiny clusters. |
| **Rand Index** | Pairwise agreement. | Sensitive to imbalance. |
| **F‑measure** | Harmonic mean of P & R. | Needs class weights. |
| **NMI** | Mutual information between clusters & labels. | 0 – 1 scale. |
| **V‑measure** | Harmonic mean of Homogeneity & Completeness. | Robust to cluster count. |
| **Topic Coherence** | Semantic relatedness of top words. | Needs a reference corpus (usually). |
