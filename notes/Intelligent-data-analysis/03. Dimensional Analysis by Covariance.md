# Dimensional Analysis by Covariance (Eigendecomposition & PCA)

[Listen to the AI-generated overview podcast on this lecture!](https://notebooklm.google.com/notebook/09cf60e3-9370-4962-b1d2-c0a2c004eeae/audio)


This lecture explores the mathematical and conceptual frameworks underlying point clouds, regression analysis, covariance, eigendecomposition, and Principal Component Analysis (PCA). These tools are foundational for dimensionality reduction, data visualization, and regression modeling.

---

## 1. Introduction to Point Clouds
<img src="https://images.ctfassets.net/26961o1141cc/1ntbH068mqsmzD1v7P69hy/d6a023bdc9027478dca19a2b49c66b82/p6.png?w=2400&h=1000&fm=webp&q=100" alt="Nature of Data" width="800">

### 1.1 What is a Point Cloud?  
A **point cloud** is a discrete set of points within a vector space, often sampled from a larger Euclidean space. Formally:
```text
P = {p} ⊆ V
```
- The **space** (`V`) can be dense or discrete, but the point cloud is always discrete.  
- Typically, point clouds are defined over **Euclidean spaces**.

### 1.2 Euclidean Space  
An **Euclidean space** is a finite-dimensional metric vector space of all `n-tuples` of real values (`Rⁿ`).  
Key properties:
- Supports vector addition and scalar multiplication.  
- Distances are defined via the **dot product**:  
  ```text
  a·b = |a||b|cos(θ)
  ```

### 1.3 Visualization  
In practice, point clouds are often visualized as 2D or 3D **scatterplots**, where each axis represents a variable. These plots can include:  
- **Trend lines**: Representing assumed patterns.  
- **Dispersion indicators**: Standard deviations or confidence intervals.

---

## 2. Analysis of Regression

### 2.1 Key Concepts  
Regression analysis is used to estimate relationships between variables.  
- **Regression Analysis**: Defines the type of relationship (e.g., linear) and produces an equation to describe it.  
- **Correlation Analysis**: Measures the strength of the relationship.

### 2.2 General Linear Model (GLM)  
The simplest regression model is a **linear model**:
```text
y = β₁x + β₀ + ε
```
- `β₁`: Slope (rate of change).  
- `β₀`: Intercept.  
- `ε`: Error term.  

### 2.3 Matrix Representation  
For multiple regressors and observations, the system is expressed in matrix form:
```text
Y = Xβ + ε
```
Solution using **Ordinary Least Squares (OLS)**:
```text
β̂ = (XᵀX)⁻¹XᵀY
```

---

## 3. Covariance

### 3.1 What is Covariance?  
**Covariance** measures how two variables co-fluctuate:
```text
σₓᵧ = E[(X - E[X])(Y - E[Y])]
```

### 3.2 Sample Estimator  
For a sample:
```text
sₓᵧ = (1 / (n - 1)) Σ[(xᵢ - x̄)(yᵢ - ȳ)]
```

### 3.3 Properties  
- Positive covariance (`sₓᵧ > 0`): Variables grow together.  
- Negative covariance (`sₓᵧ < 0`): When one variable increases, the other decreases.  

---

## 4. Eigendecomposition

### 4.1 What is Eigendecomposition?  
**Eigendecomposition** represents a matrix in terms of its eigenvalues and eigenvectors:
```text
A = QΛQ⁻¹
```
- `Λ`: Diagonal matrix of eigenvalues.  
- `Q`: Matrix of eigenvectors.

### 4.2 Key Properties  
1. Eigenvectors maintain their direction under a transformation.  
2. Eigenvalues indicate the scaling factor along their respective eigenvector directions.

### 4.3 Applications  
- Used in PCA, data compression, and dimensionality reduction.  

---

## 5. Principal Component Analysis (PCA) and Classical Multidimensional Scaling (cMDS)

### 5.1 Overview  
PCA and cMDS are dual methods for dimensionality reduction:  
- **PCA**: Starts from point coordinates.  
- **cMDS**: Starts from pairwise distances.

### 5.2 PCA Procedure  
1. Center the data:
   ```text
   X' = X - μ
   ```
2. Compute the covariance matrix:
   ```text
   Cov(X') = X'X'ᵀ / (m - 1)
   ```
3. Perform eigendecomposition or SVD to find principal components.

### 5.3 Dimensionality Reduction  
- Retain principal components with the largest eigenvalues (explaining the most variance).  

---

## 6. Dimensional Analysis by Covariance

### 6.1 Explicit vs Implicit Dimensionality  
- **Explicit Dimensionality**: Number of variables (e.g., 3D).  
- **Implicit Dimensionality**: The effective dimensionality of the data (e.g., 2D plane within a 3D space).

### 6.2 Example  
Eigenvalues from PCA:
```text
Λ = [41.13, 9.96, 0.27]
```
Normalized eigenvalues:
```text
λ̂ = [0.80, 0.19, 0.01]
```
The first two components explain 99% of the variance, suggesting the data lies on a 2D manifold.


---

### References  
1. Imola K. Fodor, "A Survey of Dimension Reduction Techniques" (2002)  
2. Roger A. Horn & Charles R. Johnson, "Matrix Analysis" (2012)  
3. Jonathon Shlens, "A Tutorial on Principal Component Analysis" (2005)
