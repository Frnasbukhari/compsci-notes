# Dimensional Analysis by Covariance (Eigendecomposition & PCA)

This lecture explores the mathematical and conceptual frameworks underlying point clouds, regression analysis, covariance, eigendecomposition, and Principal Component Analysis (PCA). These tools are foundational for dimensionality reduction, data visualization, and regression modeling.

---

## 1. Point Cloud and Euclidean Spaces
<img src="https://images.ctfassets.net/26961o1141cc/1ntbH068mqsmzD1v7P69hy/d6a023bdc9027478dca19a2b49c66b82/p6.png?w=2400&h=1000&fm=webp&q=100" alt="Point Cloud" width="800">

### 1.1 What is a Point Cloud?

A **point cloud** is a discrete set of points within a vector space, often sampled from a larger Euclidean space. Formally:
```text
P = {p} ⊆ V
```

#### Key Characteristics:
- **Discrete Nature**: While the underlying vector space (`V`) can be dense or discrete, the point cloud itself is always discrete.
- **Euclidean Space**: Although not strictly required by definition, point clouds are typically defined over Euclidean spaces for practical purposes.
- **Purpose**: A point cloud often represents a rule or association between observed variables.

---

### 1.2 Euclidean Space

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/69/Coord_system_CA_0.svg/1200px-Coord_system_CA_0.svg.png" alt="Euclidean Space" width="400">  

#### Definition:
A **Euclidean space** (or Cartesian space) is a finite-dimensional metric vector space of all `n-tuples` (i.e., vectors) of real values. Denoted as `Rⁿ` (where `n ∈ N`), it forms the foundation of modern geometry and algebra.

#### Properties:
1. **Vector Space Operations**:
   - **Addition**: Vectors can be added.
   - **Scalar Multiplication**: Vectors can be scaled by multiplying them with scalars.

2. **Metric Structure**:
   - Derived from the **inner product** (dot product):
     ```text
     a·b = |a||b|cos(θ)
     ```
   - Enables computation of distances and angles.

3. **Coordinate Systems**:
   - Commonly represented using the **Cartesian system**, where points are described as `(x₁, x₂, ..., xₙ)`.
   - Alternate systems like **polar coordinates** can also be used.

> **Example:**   
> In a 2D space (`R²`), the coordinates `(3, 4)` represent a point located 3 units along the x-axis and 4 units along the y-axis.

---

### 1.3 Visualization of Point Clouds

<img src="https://d20khd7ddkh5ls.cloudfront.net/desmos-graph_20_14.png" alt="Point clouds" width="400">  

Point clouds are often visualized in 2D or 3D using **scatterplots**, where each axis corresponds to a variable.

#### Characteristics of Scatterplots:
1. **Variable Representation**:
   - Each axis typically represents a different variable.

2. **Trend Lines**:
   - Often, scatterplots include trend lines to represent assumed patterns in the data.

3. **Dispersion Indicators**:
   - Elements like **standard deviation** or **confidence intervals** show the spread of data around the trend line.

---

#### A trend line does not imply:
- A definitive relationship between variables.
- That the represented trend is optimal.
- That the trend is a good fit.

---

#### Example:
Imagine a scatterplot of height (x-axis) and weight (y-axis) for a group of people:
- A **trend line** might indicate a general positive relationship (taller people tend to weigh more).
- **Dispersion indicators** could show the variability of weight for people of the same height.

---

## 2. Analysis of Regression
<img src="https://www.alchemer.com/wp-content/uploads/2019/04/regression-analysis-1.png" alt="Regression" width="400">  

### 2.1 Key Concepts

Regression analysis is a powerful set of techniques used to estimate relationships between variables. It can be broadly classified into two key types:

1. **Regression Analysis**:
   - Defines the **type of relationship** (e.g., linear, exponential, logarithmic, hyperbolic) between variables.
   - Produces an **equation** that describes the relationship.
   - Applications:
     - Modeling relationships between variables.
     - Predicting new outcomes based on observed data.

2. **Correlation Analysis**:
   - Measures the **strength** of the relationship between variables.
   - Produces a **value** (e.g., Pearson’s correlation coefficient) that summarizes the degree of association.

#### Example:
- **Regression**: Predicting house prices based on square footage, number of bedrooms, etc.
- **Correlation**: Assessing whether higher square footage correlates with higher house prices.

---

### 2.2 General Linear Model (GLM)
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20231129130431/11111111.png" alt="GLM" width="400">  

The most common regression approach involves the **General Linear Model (GLM)**, particularly in the form of **linear regression**.

#### Equation:
For a simple linear regression:
```text
y = β₁x + β₀ + ε
```
Where:
- `y`: Dependent variable (response).
- `x`: Independent variable (predictor).
- `β₁`: Slope (rate of change).
- `β₀`: Intercept (value of `y` when `x = 0`).
- `ε`: Error term (difference between predicted and actual values).

---

#### Uncertainty:
- **Deterministic Model**: `y = β₁x + β₀`.
- **Stochastic Model**: `E[Y] = β₁x + β₀` (includes uncertainty).
Which can be explicitly expressed as:
```text
Yᵢ = β₁Xᵢ + β₀ + εᵢ
```
Where:
- `εᵢ`: Represents the error term for the i-th observation.
- Error is the difference between the actual value and its expected value:
```text
εᵢ = Yᵢ - E[Y]
```

  
> **Note on Notation:**  
> - Standard form: `y = β₁x + β₀`.
> - General form: `ax + by + c = 0` (useful for viewing point clouds as surfaces).

---

### 2.3 Matrix Representation

When dealing with multiple variables and observations, linear regression is often expressed in matrix form:
```text
Y = Xβ + ε
```

#### Components:
- `Y`: Vector of dependent variable observations.
- `X`: Design matrix containing independent variable values.
- `β`: Coefficient vector (parameters to be estimated).
- `ε`: Error term vector.

For example, with `n` observations and `j` independent variables:
```text
Y = [            X = [                      β = [              ε = [
  Y₁               X₁j ... X₁₁ 1              βj                 εj
  Y₂               X₂j ... X₂₁ 1              ...                ε₁
  ...              ...                        β₁                 ...
  Yn               Xnj ... Xn₁ 1              β₀                 εn
]                ]                            ]                 ]
```

#### Solving the System:
The solution using **Ordinary Least Squares (OLS)** is:
```text
β̂ = (XᵀX)⁻¹XᵀY
```
This is known as the **normal equations**.

#### Practical Considerations:
- Ensure proper assumptions are met (e.g., noise independence, error normality).
- Avoid overfitting, especially in multiple regression, by carefully selecting predictors.

#### Example:
For predicting house prices based on square footage (`x₁`) and number of bedrooms (`x₂`):
```text
Y = β₁x₁ + β₂x₂ + β₀ + ε
```
The coefficients `β₁`, `β₂`, and `β₀` are estimated using the OLS method.

---

### 2.4 Residuals
<img src="https://www.statology.org/wp-content/uploads/2020/12/residuals2.png" alt="Residuals" width="400">  

#### Definition:
Residuals are the difference between the observed and predicted values:
```text
eᵢ = Yᵢ - Ŷᵢ
```

#### Usage:
- Evaluate model performance.
- Identify patterns indicating model inadequacy.

#### Example:
If a model predicts a house price of $300,000, but the actual price is $320,000:
```text
eᵢ = $320,000 - $300,000 = $20,000
```

---

### Analysis of Regression Insights

1. **Choosing the Model**:
   - Start with simple models (e.g., linear regression).
   - Evaluate alternatives (e.g., polynomial regression) based on the dataset.

2. **Avoiding Overfitting**:
   - Be cautious with multiple regression:
     - Including too many predictors increases the chance of spurious significance.
       
---

## 3. Correlation analysis
<img src="https://www.simplypsychology.org/wp-content/uploads/correlation.jpg" alt="Correlation" width="400">  

As previously discussed, **correlation analysis** focuses on summarizing the strength of the relationship between variables. Covariance, however, offers a different perspective by measuring **co-dispersion**: how much two variables fluctuate together.

---

### 3.1 Dispersion 
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20230810105933/measure-of-depression.png" alt="Dispersion" width="400">  

### Dispersion ≡ Spread

The concept of **dispersion** refers to how much a variable spreads around its mean or average. Dispersion measures the variability within a single variable, while covariance extends this to measure the relationship between two variables.


#### Estimators of Dispersion:
1. **Variance** (second centered moment):
   - Example: Population variance measures the spread of data in the entire population.
   - **Standard deviation**: Square root of the variance.
2. **Range**: Difference between specific quantiles.
3. **Coefficient of Variation**: Ratio of standard deviation to the mean.
4. **Other metrics**: Depending on the context.

For multiple variables, **covariance** is a logical extension of dispersion, assessing how two variables are co-dispersed.

---

### 3.2 What is Covariance?
<img src="https://www.statisticshowto.com/wp-content/uploads/2024/06/covariance.png" alt="Covariance" width="400">  

**Covariance** measures the extent to which two variables co-fluctuate. It quantifies the degree of *joint variation* between them. The mathematical definition is:
```text
σₓᵧ = E[(X - E[X])(Y - E[Y])]
```

#### Intuition:
- When two variables tend to move together (both increase or decrease simultaneously), their covariance is **positive**.
- When one variable increases while the other decreases, the covariance is **negative**.

#### Example:
Suppose `X` represents the hours studied and `Y` represents test scores:
- If more study hours generally result in higher test scores, `σₓᵧ > 0`.
- If more study hours lead to lower test scores (unusual but possible), `σₓᵧ < 0`.

---

### 3.3 Sample Estimator

For a sample, covariance is estimated using the following formula:
```text
sₓᵧ = (1 / (n - 1)) Σ[(xᵢ - x̄)(yᵢ - ȳ)]
```
Where:
- `xᵢ`, `yᵢ`: Individual sample points for variables `X` and `Y`.
- `x̄`, `ȳ`: Sample means of `X` and `Y`.
- `n`: Number of observations.

---

### 3.4 Properties of Covariance

1. **Positive Covariance** (`sₓᵧ > 0`):
   - Indicates that as one variable increases, the other tends to increase as well.
   - Example: Height and weight typically have positive covariance.

2. **Negative Covariance** (`sₓᵧ < 0`):
   - Indicates that as one variable increases, the other tends to decrease.
   - Example: Speed of travel and time taken for a journey.

3. **Covariance of a Variable with Itself**:
   - The covariance of a variable with itself is its variance:
     ```text
     sₓₓ = (1 / (n - 1)) Σ[(xᵢ - x̄)²]
     ```
     
---

### 3.5 Limitations of Covariance

1. **Dependency on Scale**:
   - Covariance depends on the units of measurement, making it unsuitable for comparing relationships across different datasets.
   - **Solution**: Normalize using correlation coefficients (e.g., Pearson's correlation coefficient).

2. **Magnitude Misinterpretation**:
   - A larger covariance does not necessarily imply a stronger relationship, as it is not normalized.

---

### 3.6 Applications of Covariance

1. **Correlation Analysis**:
   - Covariance forms the basis of correlation, which normalizes it to provide a measure of association between variables.

2. **Dimensional Analysis**:
   - Covariance is critical in techniques like **Principal Component Analysis (PCA)** for identifying patterns and reducing dimensionality.

3. **Modeling Relationships**:
   - Used in regression and other statistical models to understand variable dependencies.


---


## 4. Eigendecomposition
<img src="https://miro.medium.com/v2/resize:fit:710/1*datP69fmstsHSvwVof5g5g.png" alt="Eigendecomposition" width="400">  

### 4.1 What is Eigendecomposition?
An important tool in matrix algebra is **factorization**. Factorization is the process of decomposing a given matrix `A` into a product of other matrices. This technique underlies many applications in computational mathematics, including eigendecomposition.

**Eigendecomposition** is a matrix factorization technique that expresses a matrix in terms of its eigenvalues and eigenvectors. The eigendecomposition of a matrix `A` is given by:
```text
A = QΛQ⁻¹
```
Where:
- `Q`: A matrix where each column is an eigenvector of `A`.
- `Λ` (Lambda): A diagonal matrix containing the eigenvalues of `A` on the diagonal.
- `Q⁻¹`: The inverse of the matrix `Q`, assuming `Q` is invertible.

<img src="https://www.mathsisfun.com/algebra/images/eigen-transform.svg" alt="Eigenvectors" width="400">  

#### Intuition:
- **Eigenvectors**: Vectors that maintain their direction after a transformation by matrix `A`.
- **Eigenvalues**: Scalars indicating the scaling factor along the eigenvector's direction.

Eigendecomposition provides a **canonical representation** of matrices, making it a powerful tool in linear algebra and data analysis.

---

### 4.2 Understanding Eigenvectors and Eigenvalues

#### Key Idea:
For a given square matrix `A`:
```text
Aq = λq
```
Where:
- `q`: Eigenvector (direction remains unchanged).
- `λ`: Eigenvalue (scaling factor along eigenvector `q`).

Rewritten as:
```text
(A - λI)q = 0
```
This equation shows that eigenvalues and eigenvectors describe invariant transformations.

####  Note:  
- Eigenvectors maintain their direction under transformation by matrix `A`. However, this property is **unique to specific vectors** and does not apply to all vectors in the space.
- The figure below illustrates how only certain directions (eigenvectors) remain invariant, while others change direction.

<img src="https://media5.datahacker.rs/2020/03/Picture10-11-1024x1005.jpg" alt="Visualization of Eigenvectors" width="400">


---

### 4.3 Example of Eigendecomposition

#### Given Transformation Matrix:
```text
A = [3  1]
    [0  2]
```

#### Step 1: Solve for Eigenvalues
Compute  
```text
det(A – λI)
 = det([3−λ  1]
       [ 0   2−λ])
 = (3−λ)(2−λ) − 0·1
 = λ² − 5λ + 6
```

Note that 
```text
I = [1  0]
    [0  1]
```

Next, Set equal to zero and factor:  
```text
λ² − 5λ + 6 = 0
(λ − 2)(λ − 3) = 0
⇒ λ₁ = 2,  λ₂ = 3
```  

#### Step 2: Solve for Eigenvectors
Solve $(A – λI)q = 0$ for each λ.

**a) For λ₁ = 2**  
```text
A − 2I = [3−2  1]   = [1  1]
         [ 0   2−2] = [0  0]
```  
Equation:  
```text
1·x + 1·y = 0  ⇒  y = −x
```  
Choose x = 1 ⇒  
```text
q₁ = [1; −1]
```

**b) For λ₂ = 3**  
```text
A − 3I = [3−3  1]   = [0  1]
         [ 0   2−3] = [0 −1]
```  
Equation:  
```text
1·y = 0  ⇒  y = 0
```  
x is free, choose x = 1 ⇒  
```text
q₂ = [1; 0]
```

#### Step 3: Form Q and Λ
```text
Q = [ q₂  q₁ ] = [ 1  1 ]
                [ 0 −1 ]

Λ = [ 3  0 ]
    [ 0  2 ]
```

#### Step 4: Compute Q⁻¹

<img src="https://mathsathome.com/wp-content/uploads/2024/01/Formula-for-the-inverse-of-a-2x2-matrix-1024x578.png" alt="Inverse of matrix" width="400">

Here $a=1,b=1,c=0,d=-1$, so $\det Q = (1)(-1)-(1)(0)=-1$, and  
```text
Q⁻¹ = 1/(-1) · [ -1  -1 ]
               [  0   1 ]
     = [  1   1 ]
       [  0  -1 ]
```

#### Step 5: Verify $A = Q\,Λ\,Q^{-1}$
1. Compute $QΛ$:  
   ```text
   [  1   1 ]  [  3   0 ]  =  [  3   2 ] 
   [  0  -1 ]  [  0   2 ]     [  0   -2 ] 
      ```
2. Then $(QΛ)\,Q⁻¹$:  
   ```text
   [ 3  2 ]  [ 1  1 ]  =  [ 3·1+2·0     3·1+2·(−1)    ]  =  [3  1]  = A
   [ 0 -2 ]  [ 0 -1 ]     [ 0·1+(−2)·0  0·1+(−2)·(−1) ]     [0  2]
   ```
This confirms the eigendecomposition $A = Q\,Λ\,Q^{-1}$.

### 4.4 Key Remarks

1. For a given matrix `A`, there are at most `n` eigenvectors, where `n` is the rank of the matrix (number of linearly independent rows or columns).

2. Eigenvectors along the same direction are infinite since any scalar multiple of an eigenvector is also an eigenvector. However, each eigenvector corresponds to a **unique eigenvalue**, serving as its normalizing scale.

3. Eigendecomposition is only possible for **diagonalizable matrices**:
   - A matrix `A` is diagonalizable if there exists an invertible matrix `P` and a diagonal matrix `D` such that:
     ```text
     P⁻¹AP = D
     ```
   - Diagonalizable matrices are always square, but not all square matrices are diagonalizable (e.g., defective matrices).


---


## 5. Principal Component Analysis (PCA) and Classical Multidimensional Scaling (cMDS)
<img src="https://numxl.com/wp-content/uploads/analisis-de-componentes-principales-acp-102-destacada.png" alt="PCA" width="400">

### 5.1 Overview

**PCA** (Principal Component Analysis) and **cMDS** (Classical Multidimensional Scaling) are dual methods for **dimensionality reduction**. They provide insights into high-dimensional data by reducing its dimensionality while preserving as much variance or pairwise distance as possible. They idea is simple *rotate axis to align with the maximum variance*.

#### Key Differences:
- **PCA**:
  - Starts from point coordinates.
  - Based on the eigen-decomposition of the covariance or correlation matrix.
- **cMDS**:
  - Starts from pairwise distances.
  - Yields the same solution as PCA for appropriate datasets.

---

### 5.2 PCA Steps

#### Step 1: Center the Data (Mean correct)
Center the dataset by subtracting the mean:
```text
X' = X - μ
```
Where:
- `X`: Original dataset.
- `μ`: Mean vector of the dataset.

#### Step 2: Compute the Covariance Matrix
Calculate the covariance matrix of the centered data:
```text
Cov(X') = X'(X')ᵀ / (n - 1)
```
Where (X')ᵀ is the transpose.

> The transpose of a matrix just means fliping rows and columns.

#### Step 3: Perform Eigendecomposition or SVD
- Compute the eigenvalues and eigenvectors of the covariance matrix.
- The eigenvectors form the **principal components**, which are the new axes that align with the maximum variance.

---

## 6. Dimensional Analysis by Covariance
<img src="https://www.davidzeleny.net/anadat-r/lib/exe/fetch.php/obrazky:pca_algorithm_3d.jpg?w=639&tok=3705c5" alt="Dimensionality" width="400">

### 6.1 Explicit vs Implicit Dimensionality    
- **Explicit Dimensionality**: Refers to the number of variables or dimensions in the dataset. For example, a 3D point cloud has explicit dimensionality of 3.
- **Implicit Dimensionality**: Represents the effective or intrinsic dimensionality of the data, often lower than the explicit dimensionality. For example, a 3D point cloud may lie roughly on a plane, suggesting an implicit dimensionality of 2.

#### Intuition:
Imagine a point cloud in 3D:
- While it appears to have 3 dimensions explicitly, the data may lie on a 2D plane.
- Implicit dimensionality captures this reduced representation.

#### Challenge:
For higher-dimensional data:
- **How can we guess the implicit dimensionality?**
- **How do we choose a new coordinate basis to minimize loss of information?**

#### Solution:
- Techniques like **PCA** and **cMDS** help estimate the intrinsic dimensionality and reduce dimensionality while preserving variance.

---

### 6.2 Example: Dimensional Analysis with PCA
<img src="https://365datascience.com/resources/blog/2019-12-pca2-1024x348.png" alt="Dimensionality" width="400">

#### Given Dataset:
The covariance matrix's eigenvalues are:
```text
Λ = [41.13, 9.96, 0.27]
```

#### Step 1: Normalize the Eigenvalues
Normalize the eigenvalues to compute the proportion of explained variance:
```text
λ̂ = [0.8008, 0.1940, 0.0053]
```

#### Step 2: Analyze Variance
- The first two eigenvalues account for over **99% of the variance**:
  ```text
  0.8008 + 0.1940 ≈ 0.9948
  ```
- This indicates that the implicit dimensionality of the dataset is closer to **2D** than **3D**.

#### Interpretation:
The dataset effectively lies on a **2D plane** within the 3D space, reducing the need to analyze the data in 3 dimensions.

---

### 6.3 Dimensionality Reduction with PCA

Dimensionality reduction is achieved by discarding eigenvectors associated with smaller eigenvalues. These eigenvectors contribute minimally to the total variance.

### Filtering in PCA Space:
Filtering in the PCA or cMDS space is analogous to filtering in Fourier space:
- **Low-pass filter**: Set coefficients of eigenvectors with lower eigenvalues to zero, then reconstruct the data.
- **High-pass filter**: Set coefficients of eigenvectors with higher eigenvalues to zero, then reconstruct.
- **Band-pass and band-rejection filters**: Set coefficients of extreme or middle eigenvectors to zero, respectively, and reconstruct.
