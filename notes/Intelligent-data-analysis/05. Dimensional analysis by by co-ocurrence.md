
# Dimensional Analysis by Co‑Occurrence  
## Annotated Lecture Notes on Latent Semantic Analysis (LSA)

> **Original lecturer**: *Felipe Orihuela‑Espina*  
> **Session date**: 28‑Oct‑2024  
> **Value‑add editor**: <AI assistant> – translating academic detail into board‑room‑ready clarity.

---

### Table of Contents
1. [LSA – Problem Statement](#1-lsa--problem-statement)  
   1.1 [Credentials & Origin Story](#11-credentials--origin-story)  
   1.2 [PCA Recap & Gaps](#12-pca-recap--gaps)  
   1.3 [From Covariance to Co‑Occurrence](#13-from-covariance-to-cooccurrence)  
   1.4 [Document–Term Matrix](#14-documentterm-matrix)  
   1.5 [Strategic Goal of LSA](#15-strategic-goal-of-lsa)  
2. [Complex–Number Refresher](#2-complexnumber-refresher)  
3. [Conjugate‑Transpose & Unitary Matrices](#3-conjugatetranspose--unitary-matrices)  
4. [Matrix Inversion](#4-matrix-inversion)  
   4.1 [What Is an Inverse Matrix?](#41-what-is-an-inverse-matrix)  
   4.2 [Adjoint Method – Step by Step](#42-adjoint-method--step-by-step)  
5. [Singular Value Decomposition (SVD)](#5-singular-value-decomposition-svd)  
6. [LSA – The Operational Solution](#6-lsa--the-operational-solution)  
7. [References](#7-references)  

---

## 1. LSA – Problem Statement

Latent Semantic Analysis (LSA) is a **strategic capability** that transforms a raw corpus into an insight‑rich *semantic space*.  
First shipped in a 1990 landmark paper by Deerwester *et al.*, the technique leverages $ 	extbf{SVD} $ rather than traditional eigen‑decomposition to **unlock hidden relational capital** between documents and terms.citeturn0file0

### 1.1 Credentials & Origin Story
* **First formulated:** *1990*, before “big data” was even on the roadmap.  
* **Math backbone:** SVD was already a mature asset on the theoretical balance sheet.  
* **Cross‑domain utility:** Because the engine is pure linear algebra, LSA ports seamlessly from NLP to recommender systems, image tagging, and any arena where “events” co‑occur.

### 1.2 PCA Recap & Gaps
Principal Component Analysis (PCA) consumes a **square covariance matrix** of *m* observations by *n* variables.

|            | $x_1$ | $x_2$ | ⋯ | $x_n$ |
|------------|-------|-------|---|-------|
| **obs 1**  | $x_{11}$ | $x_{12}$ | ⋯ | $x_{1n}$ |
| **obs 2**  | $x_{21}$ | $x_{22}$ | ⋯ | $x_{2n}$ |
| **⋮**      | ⋮ | ⋮ | ⋯ | ⋮ |
| **obs m**  | $x_{m1}$ | $x_{m2}$ | ⋯ | $x_{mn}$ |

*Pain point:* Covariance focuses on *variance around the mean*. When distributions are multi‑modal or sparse, covariance under‑delivers.

### 1.3 From Covariance to Co‑Occurrence
LSA pivots to a **term–document co‑occurrence** approach, counting **frequency** rather than variance. This means we can ingest a tall‑and‑skinny matrix where *rows = terms* and *columns = documents* without needing a square footprint.

### 1.4 Document–Term Matrix  

|          | **doc 1** | **doc 2** | **doc 3** |
|----------|-----------|-----------|-----------|
| **term “data”**   | 4 | 0 | 1 |
| **term “cloud”**  | 1 | 3 | 0 |
| **term “matrix”** | 0 | 2 | 5 |

In column vector form, a document $d_j$ lives in term‑space as  

$ d_j = \begin{bmatrix} x_{1j} \\ x_{2j} \\ \vdots \\ x_{mj} \end{bmatrix} $  

*(Rendered here with $…$ for GitHub compatibility.)*

### 1.5 Strategic Goal of LSA
> *“Construct a semantic space wherein terms and documents that are closely associated sit near one another.”* – Deerwester et al.

We drive **dimensionality reduction** so that cosine similarity in this latent space approximates real‑world conceptual proximity—*a competitive differentiator for search relevance and recommendation engines*.

---

## 2. Complex‑Number Refresher

In the complex plane, the **Argand basis** comprises two orthogonal vectors:  
$e_1 = 1$ (real axis) and $e_2 = i$ (imaginary axis).  
Any point $p = \omega_1 e_1 + \omega_2 e_2 = \omega_1 + \omega_2 i$.

*Business takeaway:* Complex numbers encode rotation and scaling in one neat package—exactly the trick SVD leverages.

---

## 3. Conjugate‑Transpose & Unitary Matrices

* **Complex conjugate:** For $z = a + b i$, the conjugate is $z^{\*} = a - b i$. Multiplying $z$ by $z^{\*}$ yields the *real* scalar $a^2 + b^2$.  
* **Conjugate transpose (Hermitian):** Given matrix $A$, $A^{\*}$ means *transpose then conjugate each entry*.  
* **Unitary matrix:** Square matrix $U$ with $U^{\*}U = I$. Think of it as a *lossless rotation*—no scaling, no shearing.

---

## 4. Matrix Inversion

### 4.1 What Is an Inverse Matrix?
For square $A$, the inverse $A^{-1}$ satisfies $AA^{-1} = A^{-1}A = I$.  
Only *nonsingular* (determinant $
eq 0$) square matrices make the cut.

### 4.2 Adjoint Method – Step by Step
1. **Compute determinant** $|A|$.  
2. **Co‑factor matrix** $\text{cof}(A)$ – each entry is a signed minor.  
3. **Adjoint** $\text{adj}(A) = \text{cof}(A)^{\top}$.  
4. **Divide** element‑wise:  
   $A^{-1} = \dfrac{\text{adj}(A)}{|A|}$.  

**Quick 2×2 example**

If $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$:  

$|A| = ad - bc$  

$A^{-1} = \dfrac{1}{ad-bc}\begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$

---

## 5. Singular Value Decomposition (SVD)

For any $m \times n$ matrix $A$ we factor:

$A = U \Sigma V^{\*}$  

| Component | Shape | Core Function |
|-----------|-------|---------------|
| $U$ | $m \times m$ | **Unitary rotation** of row space |
| $\Sigma$ | $m \times n$ diag | **Scaling** along orthogonal axes (singular values $\sigma_i$) |
| $V^{\*}$ | $n \times n$ | **Unitary rotation** of column space |

*Key insights*

* Always exists, square or not.  
* Singular values are **non‑negative**, therefore stable for ranking importance.  
* If $A$ is square and symmetric, SVD collapses to eigen‑decomposition.

---

## 6. LSA – The Operational Solution

1. **Build occurrence matrix** $X$ (terms × documents).  
2. **Apply truncated SVD**: keep top *k* singular values – the low‑rank approximation $X_k$.  
3. **Project** both documents and terms into this $k$‑dimensional space:  
   * document vector $d_j' = \Sigma_k^{-1} U_k^{\*} x_j$  
4. **Similarity computations** (dot‑product or cosine) are now faster, noise‑reduced, and semantically richer.

*Enterprise impact:* This pipeline condenses high‑entropy text data into a set of **actionable latent features** – the very definition of *data monetization*.

---

## 7. References

1. Deerwester, S. T., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990). *Indexing by latent semantic analysis*. **JASIS**, 41(6), 391‑407.  
2. Ben‑Israel, A., & Greville, T. N. E. (2006). *Generalized inverses: theory and applications*. Springer.  
3. Penrose, R. (1955). *A generalized inverse for matrices*. **Proc. Cambridge Phil. Soc.**, 51, 406‑413.

---

*These notes were autogenerated for seamless GitHub rendering. Math is wrapped in dollar signs, and matrices are expressed as simple tables to keep your Markdown linting pipeline green.*

