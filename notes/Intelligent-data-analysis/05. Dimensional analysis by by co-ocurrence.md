# Dimensional Analysis by Co‑occurrence — *Latent Semantic Analysis*

---

## 1 — Latent Semantic Analysis (Problem Statement)

### 1.1  LSA credentials
Latent Semantic Analysis (LSA) surfaced in 1990 as a technique for **discovering latent concepts hidden in a document‑term co‑occurrence matrix**. At its core, LSA is just a *Singular Value Decomposition* (SVD) applied to the count matrix, so it travels well beyond text — any tall, rectangular data set with meaningful co‑occurrences can ride the same math.

Key takeaways:

| Fact | Insight |
|------|---------|
| SVD predates LSA by decades | LSA is an *application* of SVD, not a new algorithm |
| “Latent Semantic Indexing (LSI)” in IR | Same idea, different community |
| Handles non‑square data | Unlike PCA, which starts from a square covariance matrix |

### 1.2  From PCA to LSA — why co‑occurrence beats covariance for text

1. **PCA recap**:  
   *Data matrix* $\mathbf{X} \in \mathbb{R}^{m \times n}$ (rows = observations, columns = variables)  
   Covariance $\mathbf{A}= \text{cov}(\mathbf{X})$ is $n\times n$, symmetric, and amenable to eigendecomposition.

2. **Problem**: For text, rows (terms) and columns (documents) are *different species*. A square covariance matrix would force them into the same type, losing information.

3. **Solution**: Keep the raw *occurrence* matrix and factorise it directly with SVD.

### 1.3  Document–Term Matrix anatomy

*Notation from the slides:*

| | $d_1$ | $d_2$ | … | $d_n$ |
|---|---|---|---|---|
| **$t_1$** | $x_{11}$ | $x_{12}$ | … | $x_{1n}$ |
| **$t_2$** | $x_{21}$ | $x_{22}$ | … | $x_{2n}$ |
| … | … | … | … | … |
| **$t_m$** | $x_{m1}$ | $x_{m2}$ | … | $x_{mn}$ |

* Columns $d_j$ are **document vectors**.  
* Rows $t_i$ are **term vectors**.  
* Entries $x_{ij}$ are raw counts, TF‑IDF scores, or any weighting scheme.

### 1.4  Goal statement

> “Construct a semantic space wherein terms and documents that are closely associated lie near one another.”  

SVD arranges that space; cosine similarity between the low‑rank vectors is our semantic proximity metric.

---

### 1.5  What we already know from the module

#### Coordinate bases refresher (Argand diagram)

A complex number $p = \omega_1 + \omega_2 i$ can be decomposed in the basis $\{e_1=(1,0)^T,\;e_2=(0,i)^T\}$:

$$p = \omega_1 e_1 + \omega_2 e_2$$

Same idea generalises: choose a basis, express vectors as weighted sums of those basis vectors. LSA will **learn** a new basis that captures semantics.

#### Eigendecomposition cheat‑sheet

For a *square*, symmetrical matrix $\mathbf{A}$:

$$ \mathbf{A} = \mathbf{P}\,\mathbf{D}\,\mathbf{P}^{-1} $$

* $\mathbf{P}$ — columns are eigenvectors (basis change).  
* $\mathbf{D}$ — diagonal of eigenvalues (axis scaling).

LSA swaps eigendecomposition for SVD because $\mathbf{X}$ is rectangular. Still, the spirit — rotate to a smart basis then scale — is identical.

Example 3×3 covariance matrix (presented as a table):

| | $X_1$ | $X_2$ | $X_3$ |
|---|---|---|---|
| **$X_1$** | $\sigma^2_{X_1}$ | $\sigma_{X_1,X_2}$ | $\sigma_{X_1,X_3}$ |
| **$X_2$** | $\sigma_{X_2,X_1}$ | $\sigma^2_{X_2}$ | $\sigma_{X_2,X_3}$ |
| **$X_3$** | $\sigma_{X_3,X_1}$ | $\sigma_{X_3,X_2}$ | $\sigma^2_{X_3}$ |

---

### 1.6  Putting it all together — SVD in action

Given $\mathbf{X}$:

$$ \mathbf{X} = \mathbf{U}\,\mathbf{\Sigma}\,\mathbf{V}^T $$

* $\mathbf{U}$ — term vectors (orthonormal).  
* $\mathbf{V}$ — document vectors (orthonormal).  
* $\mathbf{\Sigma}$ — singular values (importance weights).

#### Rank‑$k$ truncation

Keep only the top $k$ singular values:

$$ \mathbf{X}_k = \mathbf{U}_k\,\mathbf{\Sigma}_k\,\mathbf{V}_k^T $$

*Reduces noise and reveals latent structure.* Typical $k$ values for IR are 100–300.

#### Worked micro‑example (toy 4×3 matrix)

| | Doc A | Doc B | Doc C |
|---|---|---|---|
| **word: cat** | 2 | 0 | 0 |
| **word: dog** | 1 | 1 | 0 |
| **word: apple** | 0 | 0 | 3 |
| **word: fruit** | 0 | 0 | 2 |

SVD captures that *cat* and *dog* cluster (pet concept), while *apple* and *fruit* cluster (food concept), even though those words never co‑occur in the same document.

---

### 1.7  Quick reference

* **Input**: raw or weighted document–term matrix.  
* **Process**: centre/weight → SVD → truncate → normalise vectors.  
* **Outputs**: low‑dimensional embeddings, similarity scores, dimension‑wise “topics”.

---

## 2 — Complex Conjugates & the Conjugate Transpose Matrix

### 2.1  Complex Conjugate — definition & example

A complex number $z = a + b i$ has *conjugate* $z^{\*} = a - b i$.

| $z$ | $z^{\*}$ |
|-----|---------|
| $3 + 4i$ | $3 - 4i$ |

Multiplying a number by its conjugate yields a non‑negative real number:

$$z\,z^{\*} = (a + b i)(a - b i)=a^2 + b^2$$

---

### 2.2  Key algebraic properties

* **Distributive**  
  $(z + w)^{\*} = z^{\*} + w^{\*}$,   $(z - w)^{\*} = z^{\*} - w^{\*}$  
  $(z w)^{\*} = z^{\*} w^{\*}$,   $\bigl(\dfrac{{z}}{{w}}\bigr)^{\*} = \dfrac{{z^{\*}}}{{w^{\*}}}$ for $w \ne 0$

* **Modulus unchanged**  
  $|z^{\*}| = |z|$

* **Involutory**  
  $(z^{\*})^{\*} = z$

* **Powers commute**  
  $(z^n)^{\*} = (z^{\*})^n$

---

### 2.3  Why do we care?

* **Undo rotations** in the Argand plane.  
  Example: multiplying by $1+i$ rotates a vector 45°. Multiply again by its conjugate $1-i$ to rotate back.

* **Algebraic simplification** (e.g. rationalising denominators).

* **Polynomial roots**: complex roots appear in conjugate pairs.

* **Signal processing**: Fourier transforms exploit conjugate symmetry.

* **Quantum mechanics**: state vectors use conjugate transposes in inner products.

---

### 2.4  Conjugate Transpose (Hermitian Transpose)

Given $\mathbf{{A}} \in \mathbb{{C}}^{m \times n}$ with entries $z_{{ij}}$, the **conjugate transpose** $\mathbf{{A}}^{\*}$ is produced by

1. Transposing $\mathbf{{A}}$  
2. Taking the conjugate of every entry.

|      |      | … | $z_{{m1}}$ |
|------|------|----|-----------|
| **$\mathbf{{A}}$** | $z_{{11}}$ | … | $z_{{1n}}$ |
| ⋮ | ⋱ | ⋱ | ⋮ |
| $z_{{m1}}$ | … | $z_{{mn}}$ |

becomes

|      |      | … | $z_{{1m}}^{\*}$ |
|------|------|----|--------------|
| **$\mathbf{{A}}^{\*}$** | $z_{{11}}^{\*}$ | … | $z_{{m1}}^{\*}$ |
| ⋮ | ⋱ | ⋱ | ⋮ |
| $z_{{1n}}^{\*}$ | … | $z_{{mn}}^{\*}$ |

---

### 2.5  Unitary Matrices
A square complex matrix $\mathbf{{U}}$ is **unitary** if

$$\mathbf{{U}}^{\*}\mathbf{{U}} = \mathbf{{U}}\mathbf{{U}}^{\*} = \mathbf{{I}}$$

Equivalently, $\mathbf{{U}}^{-1} = \mathbf{{U}}^{\*}$ — the conjugate transpose is its own inverse.

*Real‑valued analogue*: orthogonal matrices satisfy $\mathbf{{Q}}^{\top}\mathbf{{Q}}=\mathbf{{I}}$.

*Special unitary*: add $\det(\mathbf{{U}})=1$ and you have the group $SU(n)$.

---

#### Quick Mnemonic

> **“Flip and star.”**  
> To build $\mathbf{{A}}^{\*}$, *flip* the matrix across its diagonal, then add a *star* to every entry.

---


## 3 — Matrix Inversion

### 3.1  Inverse Matrix: Concept & Motivation

An **inverse matrix** plays the same role for matrices that reciprocals play for numbers:

| Scalars | Matrices |
|---------|----------|
| $a \cdot a^{-1} = 1$ | $\mathbf{A}\,\mathbf{A}^{-1} = \mathbf{I}$ |

Key points
* Both left and right products must equal the identity: $\mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}$.  
* Only then does $\mathbf{A}^{-1}$ truly “undo” $\mathbf{A}$.

> **Why care?** We have matrix multiplication but *no defined division*.  
> To “divide” by $\mathbf{A}$, multiply by $\mathbf{A}^{-1}$ instead:  
> If $\mathbf{X}\mathbf{A} = \mathbf{B}$, then $\mathbf{X} = \mathbf{B}\mathbf{A}^{-1}$.

---

### 3.2  Existence & Uniqueness Theorems

#### Theorem 1 — Only square matrices can have an inverse  
*Proof sketch*: Assume $\mathbf{A}$ is $n\times m$ with $n\neq m$.  
Equality $\mathbf{A}\mathbf{A}^{-1}= \mathbf{I}_n$ forces $\mathbf{A}^{-1}$ to be $m\times n$;  
$\mathbf{A}^{-1}\mathbf{A}= \mathbf{I}_m$ forces the same size.  
This is only simultaneously possible if $n=m$.

#### Theorem 2 — Inverse is unique (when it exists)  
If $\mathbf{B}$ and $\mathbf{C}$ both satisfy the inverse property,  
$$\mathbf{B} = \mathbf{B}(\mathbf{A}\mathbf{C}) = (\mathbf{B}\mathbf{A})\mathbf{C} = \mathbf{C}$$

#### Theorem 3 — Existence iff $\det(\mathbf{A})\neq 0$  
The adjoint‑based formula below shows the determinant in the denominator; zero would be illegal.

---

### 3.3  Adjoint (Cofactor) Method

Although not the fastest numerically, the adjoint route is pedagogically clean.

#### 3.3.1  Cofactors & Cofactor Matrix
The **cofactor** $A_{ij}$ equals $(-1)^{i+j}$ times the determinant of the minor obtained by deleting row i and column j.

Example $3\times3$ template (sign‑adjusted):

| | $j=1$ | $j=2$ | $j=3$ |
|---|---|---|---|
| **$i=1$** | $A_{11}=a_{22}a_{33}-a_{23}a_{32}$ | $A_{12}=a_{23}a_{31}-a_{21}a_{33}$ | $A_{13}=a_{21}a_{32}-a_{22}a_{31}$ |
| **$i=2$** | $A_{21}=a_{13}a_{32}-a_{12}a_{33}$ | $A_{22}=a_{11}a_{33}-a_{13}a_{31}$ | $A_{23}=a_{12}a_{31}-a_{11}a_{32}$ |
| **$i=3$** | $A_{31}=a_{12}a_{23}-a_{13}a_{22}$ | $A_{32}=a_{13}a_{21}-a_{11}a_{23}$ | $A_{33}=a_{11}a_{22}-a_{12}a_{21}$ |

Collect these into the **cofactor matrix** $\text{cof}(\mathbf{A})$; transpose to get the **adjoint** $\text{adj}(\mathbf{A})$.

#### 3.3.2  Key identity  
$$\mathbf{A}\,\text{adj}(\mathbf{A}) = \text{adj}(\mathbf{A})\,\mathbf{A} = \det(\mathbf{A})\mathbf{I}$$

#### 3.3.3  Inverse formula  
$$\boxed{\;\mathbf{A}^{-1} = \dfrac{\text{adj}(\mathbf{A})}{\det(\mathbf{A})}\;}$$

---

#### 3.3.4  Worked Example

Given  

| | Col 1 | Col 2 | Col 3 |
|---|---|---|---|
| **Row 1** | $8$ | $4$ | $2$ |
| **Row 2** | $3$ | $0$ | $1$ |
| **Row 3** | $2$ | $-1$ | $3$ |

1. **Determinant**: $\det(\mathbf{A})=-26$ (slide computation).  
2. **Cofactor matrix** $\text{cof}(\mathbf{A})$ (already sign‑corrected):

| | | | |
|---|---|---|---|
| $1$ | $-7$ | $-3$ |
| $-14$ | $20$ | $16$ |
| $4$ | $-2$ | $-12$ |

3. **Adjoint** (transpose):

| | | | |
|---|---|---|---|
| $1$ | $-14$ | $4$ |
| $-7$ | $20$ | $-2$ |
| $-3$ | $16$ | $-12$ |

4. **Divide each entry by $\det(\mathbf{A})$** to get $\mathbf{A}^{-1}$:

| | | | |
|---|---|---|---|
| $-\dfrac{1}{26}$ | $\dfrac{14}{26}$ | $-\dfrac{4}{26}$ |
| $\dfrac{7}{26}$ | $-\dfrac{20}{26}$ | $\dfrac{2}{26}$ |
| $\dfrac{3}{26}$ | $-\dfrac{16}{26}$ | $\dfrac{12}{26}$ |

*Challenge*: Verify $\mathbf{A}\mathbf{A}^{-1}=\mathbf{I}$.

---

### 3.4  Practical Notes

* **Adjoint method** is $O(n!)$ for determinant → unwise for $n\gt3$.  
* **Gauss‑Jordan** (augment with $\mathbf{I}$, row‑reduce) is $O(n^3)$ and numerically stabler.  
* **Moore–Penrose pseudo‑inverse** generalises the concept to non‑square or singular matrices (beyond today’s scope).

---

## 4 — Singular Value Decomposition (SVD)

### 4.1  Core Factorisation

For any $m \times n$ matrix $\mathbf{A}$ there exist  

$$\mathbf{A} = \mathbf{U}\,\boldsymbol{\Sigma}\,\mathbf{V}^{\*}$$

| Symbol | Size | Role |
|--------|------|------|
| $\mathbf{U}$ | $m \times m$ | **Left singular vectors** — columns are orthonormal |
| $\boldsymbol{\Sigma}$ | $m \times n$ | **Singular values** on the diagonal: $\sigma_{1}\ge\sigma_{2}\ge\dots\ge0$ |
| $\mathbf{V}^{\*}$ | $n \times n$ | Conjugate‑transpose of $\mathbf{V}$; columns of $\mathbf{V}$ are right singular vectors |

When $\mathbf{A}$ is real, $\mathbf{V}^{\*}$ simplifies to $\mathbf{V}^{\top}$. 

---

### 4.2  Interpretation: Rotate → Scale → Derotate

1. **$\mathbf{V}^{\*}$** rotates input coordinates to a “nice” basis.  
2. **$\boldsymbol{\Sigma}$** scales each axis by $\sigma_{i}$.  
3. **$\mathbf{U}$** rotates (and possibly reflects) into output space.

Net effect: stretch and re‑orient, just like eigendecomposition, but without the square‑matrix handcuffs.

---

### 4.3  Why the conjugate transpose?

A complex number $z=a+bi$ can be represented as the real matrix 

| | |
|---|---|
| $a$ | $-b$ |
| $b$ | $a$ |

— a bona‑fide 2‑D rotation. Conjugate transpose in SVD mirrors this logic for higher dimensions: we rotate in a unitary space, and taking the “$^{\*}$” ensures angles and lengths survive the journey.

---

### 4.4  Connection to Eigendecomposition

Because  

$$\mathbf{A}\mathbf{A}^{\*} = \mathbf{U}\,(\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\*})\,\mathbf{U}^{\*}, \qquad 
\mathbf{A}^{\*}\mathbf{A} = \mathbf{V}\,(\boldsymbol{\Sigma}^{\*}\boldsymbol{\Sigma})\,\mathbf{V}^{\*}$$

each of these *square* products is eigendecomposed by the same singular vectors. The eigenvalues are $\sigma_i^{2}$. Hence eigendecomposition is just the SVD of a matrix that happened to be square. citeturn10file0

---

### 4.5  Properties Worth Memorising

| Topic | Eigendecomposition | SVD |
|-------|-------------------|-----|
| Existence | Only if $\mathbf{A}$ is square and diagonalizable | **Always exists** |
| Vector orthogonality | Not guaranteed | **Guaranteed** |
| Values | Eigenvalues can be negative or complex | $\sigma_i \ge 0$, real |
| Inverses | $\mathbf{P}^{-1}$ is $\mathbf{P}^{\*}$ *only* for Hermitian | $\mathbf{U}^{\*}\mathbf{U} = \mathbf{I}$, $\mathbf{V}^{\*}\mathbf{V}=\mathbf{I}$ |

---

### 4.6  How to Compute SVD in Practice

> **Reality check:** hand‑cranking SVD is CFO‑level expensive—delegate to software.

* **Lanczos / Krylov methods** — fast for top $k$ singular triplets (think LSA/LSI), but watch numerical stability.  
* **One‑sided Jacobi** — iteratively orthogonalises columns.  
* **Two‑sided Jacobi (Givens rotations)** — generalises the eigen Jacobi algorithm.  
* **QR‑based routines** — industry default inside LAPACK.  

For didactic bite‑size, analytical closed forms exist for $2\times2$ matrices (see Wikipedia) but rely on Pauli matrices, out of scope here. citeturn10file0

---

#### Quick Pseudo‑code (conceptual)

```
A = input matrix
U, Σ, Vt = zeros
while not converged:
    find pivot (p,q)
    compute Jacobi rotation θ
    A = A * J(p,q,θ)
    accumulate U and Vt
extract Σ from A’s diagonal
```

---

### 4.7  Low‑Rank Approximations (LSA tie‑in)

Keep only the top $k$ singular values:

$$\mathbf{A}_k = \mathbf{U}_k \boldsymbol{\Sigma}_k \mathbf{V}_k^{\*}$$

This is the **best rank‑$k$ approximation** under the Frobenius norm—exactly what Latent Semantic Analysis exploits to uncover hidden topics.

---


## 5 — Latent Semantic Analysis: *Putting It All Together*

### 5.1  Why SVD Beats Eigendecomposition for LSA

Deerwester *et al.* (1990) rejected eigendecomposition because it assumes a **one‑mode** matrix (documents × documents).  
LSA demands a **two‑mode** analysis (terms × documents) — a rectangular matrix. Enter SVD.

| Approach | Input matrix | Limitation for text corpora |
|----------|--------------|-----------------------------|
| Eigen‑decomposition | $\mathbf{X}^{\top}\mathbf{X}$ (square) | Collapses terms and docs into the same “kind” |
| **SVD (LSA)** | $\mathbf{X}$ ($m$ terms × $n$ docs) | Handles two distinct entity types |

---

### 5.2  Low‑Rank Approximation: The Secret Sauce

LSA keeps only the top $k$ singular values:

$$\mathbf{X}_k = \mathbf{U}_k\,\boldsymbol{\Sigma}_k\,\mathbf{V}_k^{\top}$$

Motivations
* **Polysemy relief** — dimensions merge synonymous or related terms.  
* **Computation** — in the 1990s, trimming cut memory and CPU.  
* **Sparsity** — large vocabularies make $\mathbf{X}$ mostly zeros; SVD compresses the essence.

---

### 5.3  What the Latent Space *Is* (and Isn’t)

* Not designed for human‑readable “topics”.  
* **Purely geometric**: vectors in $\mathbb{R}^k$ (or $\mathbb{C}^k$) where cosine similarity captures semantic closeness.  
* Works because distributional statistics encode meaning indirectly.

---

### 5.4  Practical Applications

| Domain | How LSA Helps |
|--------|---------------|
| **Information retrieval** | Rank documents by cosine similarity in latent space; improves recall for synonymy & polysemy. |
| **Variable clustering / classification** | Group words or docs without manual labels. |
| **Feature engineering** | Feed dense latent vectors into downstream ML models. |
| **Query expansion & matching** | Map user queries to conceptually related terms, boosting search quality. |

---

### Workflow Recap

1. **Build** term‑document matrix $\mathbf{X}$ (counts or TF‑IDF).  
2. **Factorise** via SVD → $\mathbf{U}, \boldsymbol{\Sigma}, \mathbf{V}^{\top}$.  
3. **Truncate** to rank $k$.  
4. **Embed** documents (columns of $\boldsymbol{\Sigma}_k\mathbf{V}_k^{\top}$) and terms (columns of $\mathbf{U}_k\boldsymbol{\Sigma}_k$).  
5. **Compare** using dot‑product / cosine.
