# Week 2 Lecture: Nature of Data - Basis and Coordinate Systems

This lecture provides a comprehensive exploration of basis and coordinate systems, their role in data representation, and their application through various decompositions. It also highlights manifold embedding and provides guidance on the appropriate use of decompositions in data analysis.

---

## 1. Basis and Coordinate Systems

### 1.1 What is a Basis?
- A **basis** is a set of functions \( E = \{ e_i \in V \} \) that allows every element of a set \( V \) to be expressed uniquely as a linear combination of the basis functions:
  \[
  \forall v \in V: v = \sum_{i} \omega_i e_i
  \]
- \( \omega_i \): The coefficients (or weights) in the linear combination are called the **coordinates** of the element \( v \) in the chosen basis.
- The **dimension** of the set is the number of basis functions.

**Key Concept**:
Every point in the space can be uniquely represented using a chosen basis, and the tuple \( (\omega_1, \omega_2, \dots) \) defines the coordinates of the point.

---

### 1.2 Examples of Coordinate Basis

#### 1.2.1 Canonical Basis in Euclidean Space \( \mathbb{R}^n \)
- In the 2D Euclidean space \( \mathbb{R}^2 \), the canonical basis is:
  \[
  \{ (1, 0), (0, 1) \}
  \]
- Example:
  Represent the point \( p = (2.5, 1.4) \):
  \[
  p = 2.5 \cdot (1, 0) + 1.4 \cdot (0, 1)
  \]

#### 1.2.2 Argand Basis in Complex Plane \( \mathbb{C} \)
- The basis functions are:
  \[
  \{ 1, i \}
  \]
- Example:
  Represent the complex number \( z = 1.8 + 3.2i \):
  \[
  z = 1.8 \cdot 1 + 3.2 \cdot i
  \]

#### 1.2.3 Orthogonal Unit Vectors in Physics
- The familiar basis in 3D Cartesian coordinates:
  \[
  \{ \hat{i}, \hat{j}, \hat{k} \}
  \]
- Example:
  Represent the vector \( \mathbf{p} = -1.6\hat{i} + 1.5\hat{j} + 2.5\hat{k} \).

**Key Insight**:
Coordinate systems allow us to represent elements uniquely, and the choice of basis can vary depending on the context.

---

### 1.3 Representing Signals Using Descriptors

#### Signal Set Example
Consider a set of four signals defined by amplitude and frequency:

| Signal | Amplitude (A) | Frequency (F) |
|--------|---------------|---------------|
| S1     | 1             | 1             |
| S2     | 2             | 1             |
| S3     | 1             | 2             |
| S4     | 2             | 2             |

#### Dimension of the Set
- The **dimension** is the minimum number of descriptors required to uniquely identify each signal.
- Here, the dimension is 2 (amplitude and frequency).

#### Representing Signals
- Using the basis \( \{ A, F \} \), each signal is represented as:
  \[
  S_1 = (1, 1), \quad S_2 = (1, 2), \quad S_3 = (2, 1), \quad S_4 = (2, 2)
  \]
- Alternative choices for basis (e.g., swapping order or scaling) change the representation but not the uniqueness.

---

## 2. Decompositions in Data Analysis

Decompositions provide methods to express data in terms of new basis functions that simplify processing or analysis.

### 2.1 General Linear Model (GLM)

#### 2.1.1 Definition
- GLM expresses relationships between variables as:
  \[
  y = \beta_0 + \beta_1 x + \epsilon
  \]
  where \( \epsilon \) is the error term.

#### 2.1.2 Solving GLM for Two Points
- Given two points \( (x_1, y_1) \) and \( (x_2, y_2) \), the equation of the line is:
  \[
  \beta_1 = \frac{y_2 - y_1}{x_2 - x_1}, \quad \beta_0 = y_1 - \beta_1 x_1
  \]

#### 2.1.3 Multivariate GLM
- Generalizes to multiple predictors:
  \[
  y = \beta_0 + \sum_{i=1}^n \beta_i x_i
  \]
- Expressed in matrix form:
  \[
  \mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
  \]
  Solved as:
  \[
  \boldsymbol{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}
  \]

#### 2.1.4 GLM as a Point in Space
- A linear model \( y = \beta_0 + \beta_1 x \) can be visualized as a point \( (\beta_0, \beta_1) \) in the space of all lines.

---

### 2.2 Fourier Analysis

#### 2.2.1 Definition
- Decomposes a signal into a sum of sine and cosine functions:
  \[
  f(t) = \sum_{n} \left( a_n \cos(n\omega t) + b_n \sin(n\omega t) \right)
  \]

#### 2.2.2 Applications
- Filtering: Retain specific frequency components.
- Signal reconstruction: Combine components to approximate the original signal.

#### Example
- Given \( f(t) = \sin(2\pi t) + \frac{1}{3}\sin(6\pi t) \):
  - Frequency components: \( (1, \frac{1}{3}) \).
  - Low-pass filter: Set high-frequency component to 0.

---

### 2.3 Wavelet Transform

#### Definition
- Represents signals using scaled and translated wave-like functions:
  \[
  f(t) = \sum_{k,n} c_{k,n} \Psi(t; k, n)
  \]

#### Applications
- Spatiotemporal signal analysis.
- Denoising and compression.

---

### 2.4 Principal Component Analysis (PCA)

#### 2.4.1 Definition
- Rotates data to align with axes of maximum variance:
  \[
  \mathbf{X}' = \mathbf{X} - \mu
  \]

#### 2.4.2 Procedure
1. Center the data.
2. Compute the covariance matrix.
3. Perform singular value decomposition (SVD) to find principal components.

#### 2.4.3 Dimensionality Reduction
- Retain components with the largest eigenvalues (i.e., explain most variance).

---

## 3. Manifold Embedding

### 3.1 What is a Manifold?
- A manifold is a topological space that locally resembles Euclidean space, meaning each small neighborhood of the space behaves like flat, standard Euclidean geometry. Globally, however, it may have a more complex structure, such as curvature.
- **Formal Definition**: A manifold is a set (Hausdorff space) with a countable base that is homeomorphic to a Euclidean space. For every point on the manifold, there exists a local neighborhood that can be mapped to a flat space via a function.
- **Example**: A sphere is locally flat but globally curved. While a small patch on the surface of the Earth can be approximated as flat, the Earth as a whole is spherical.

**Applications in Data Representation**:
- Manifolds provide a framework for understanding high-dimensional data. For example, many real-world datasets lie on a lower-dimensional manifold within the high-dimensional space.
- Techniques like t-SNE and Isomap project high-dimensional data onto a low-dimensional manifold, preserving its structure for visualization or analysis.
- **Example in Machine Learning**: In image recognition, the set of all possible images forms a high-dimensional space, but images of specific objects (e.g., digits in MNIST) lie on a structured manifold within this space.

### 3.2 Applications
- Dimensionality reduction (e.g., t-SNE, Isomap).
- Nonlinear data structure representation.

---

## 4. Final Remarks

- Decompositions operate by changing the coordinate basis.
- Use tools appropriately:
  - GLM: Regression and experimental design.
  - Fourier: Filtering and spectral analysis.
  - Wavelets: Denoising and compression.
  - PCA/cMDS: Dimensionality reduction and visualization.
  - ICA: Source separation.
  - Manifold embedding: Complex nonlinear problems.
- Avoid using overly generic tools when simpler ones suffice.

Thank you!

