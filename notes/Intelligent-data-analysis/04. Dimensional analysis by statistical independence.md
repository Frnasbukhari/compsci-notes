# Independent Component Analysis (ICA)

## 1. Problem Statement
<img src="https://miro.medium.com/v2/resize:fit:1400/1*iSriZU7D4zikLtX0pGBqNA.png" alt="ICA" width="800">

Independent Component Analysis (ICA) is a method for decomposing a signal into statistically independent components. Originally formulated in 1986 by Herault and Jutten, ICA addresses complex problems like **blind source separation (BSS)**, with applications ranging from audio processing to neuroimaging.

### Key Characteristics of ICA:
- **Statistical Independence**: Relaxes the orthogonality assumption in PCA and substitutes it with statistical independence.
- **Applicability**: Broad adoption due to its versatility.
  
---

#### GLM vs. ICA:
The General Linear Model (GLM) is expressed as:

$$ 
y = X\beta + \varepsilon
$$

$$
\beta = (X^T X)^{-1} X^T y
$$

However, in ICA:

$$
Y = Xβ + ε
$$

Here, neither β nor X is known, making the problem far more complex.

> **Example: The Cocktail Party Problem**
> ICA can solve scenarios like separating voices in a room with multiple speakers ("cocktail party problem"). This involves identifying both the independent sources and their mixtures without prior knowledge.

---


## 2. Distributions

Distributions are mathematical functions that describe how probabilities are distributed over different outcomes. The choice of distribution depends on whether the sampling space is discrete or dense.

<img src="https://i0.wp.com/www.dataairevolution.com/wp-content/uploads/2024/07/data-distributions.png?fit=640%2C331&ssl=1" alt="Distributions" width="400">


### 2.1 Probability Function
The **Probability Function** is used for **discrete random variables**. It defines the probability that a discrete random variable $X$ takes on a specific value $x_i$.

#### Definition:
- Let $S = \{s_i\}$ be a discrete sample space, where $s_i$ are all possible outcomes in $S$. 

- Let $X : S \to T$ be a discrete random variable, with $T = \{x_i\}$, such that $x_i = X(s_i)$.

- The **probability function** $f(x_i)$ is defined as:

$$
f(x_i) = Pr(X = x_i)
$$

Where $Pr(X = x_i)$ is the probability of $X$ taking the value $x_i$.


> **The probability function can be used for:**      
> - Qualitative variables (categorical or ordinal).
> - Quantitative variables (discrete).

---

#### Example:
Consider rolling a six-sided die. The sample space is $S = \{1, 2, 3, 4, 5, 6\}$. The probability function is:

$$
f(x_i) = Pr(X = x_i) = \frac{1}{6}, \quad \text{for all } x_i \in S.
$$

This means the probability of rolling any specific number (e.g., 3) is $1/6$.

---

### 2.2 Probability Density Function (PDF)
The **Probability Density Function (PDF)** is used for dense (sometimes incorrectly referred to as continuous) variables. Unlike discrete variables, dense variables take on values within intervals rather than distinct points. The PDF characterizes the likelihood of a variable falling within a certain range.

<img src="https://cdn1.byjus.com/wp-content/uploads/2021/10/Probability-Density-Function.png" alt="PDF" width="400">

#### Definition:
The PDF is defined as:

$$
f(x) = \frac{dPr(X \leq x)}{dx}
$$

Where $f(x)$ is the density function and $Pr(X \leq x)$ is the cumulative probability up to $x$.

---

#### Properties:
1. **Non-Negativity**: $f(x) \geq 0$ for all $x$.
2. **Normalization**: The total area under the PDF curve is 1:

$$
\int_{-\infty}^{\infty} f(x) dx = 1
$$
   
3. **Not a Probability**: The PDF does not represent the probability of $X = x$, but rather the density over an interval. For instance, $Pr(a \leq X \leq b)$ is computed as:

$$
\int_a^b f(x) dx
$$

Values of $f(x) > 1$ are possible and do not violate probability rules because they represent densities, not direct probabilities.

---

#### Key Observations:
- The PDF is **not** used to calculate $Pr(X = x)$, as $Pr(X = x) = 0$ for dense variables.
- The PDF is the derivative of the CDF:
  
$$
f(x) = \frac{dF(x)}{dx}
$$

#### Example:
Imagine a random variable $X$ representing the time (in hours) it takes for a machine to complete a task. If the PDF $f(x)$ is given by a bell-shaped curve, then the likelihood of the machine completing the task between 2 and 4 hours is the area under the curve from $x = 2$ to $x = 4$.

---

### 2.3 Cumulative Probability Distribution (CDF)
The **Cumulative Probability Distribution (CDF)** provides the cumulative probability that a random variable $X$ is less than or equal to a specific value $x$.

<img src="https://study.com/cimages/multimages/16/screen_shot_2022-08-19_at_5.47.55_pm8177924067442430378.png" alt="CDF" width="400">

#### Definition:
The CDF is defined as:

$$
F(x) = Pr(X \leq x)
$$

Where $F(x)$ represents the cumulative probability up to $x$.

---

#### Properties:
1. **Limits**:
   - $F(x) \to 0$ as $x \to -\infty$.
   - $F(x) \to 1$ as $x \to \infty$.
2. **Non-Decreasing**:
   - For any $x_1 < x_2$, $F(x_1) \leq F(x_2)$.
3. $Pr(X > x) = 1 - F(x)$.
4. The CDF provides a straightforward way to compute probabilities over intervals. For example, $Pr(x_1 < X \leq x_2)$ can be calculated as:
  
$$
Pr(x_1 < X \leq x_2) = F(x_2) - F(x_1)
$$

#### Example:
Consider a random variable $X$ representing the height of students in a class. If the CDF $F(x)$ at $x = 160$ cm is 0.7, this means that 70% of students are 160 cm or shorter. The probability of a student being taller than 160 cm is $1 - F(160) = 0.3$.

---

## 3. Joint Distributions and Statistical Independence

### 3.1 Joint Distribution
The joint distribution describes the probabilities of multiple variables:

$$
P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n)
$$

### 3.2 Statistical Independence
Two variables are independent if:   

$$
P(A \cap B) = P(A)P(B)
$$
> The probability that both A and B happen together is exactly the same as the product of the probability that A happens alone and the probability that B happens alone.
> If knowing A happens doesn’t change the chances of B happening (and vice versa), they're independent.

For random variables:    

$$
P(X_1, X_2) = P(X_1)P(X_2)
$$

**Remark**: Independence implies no mutual information but not necessarily no correlation.

---

## 4. Entropy and Information Theory

### 4.1 Entropy
Entropy quantifies uncertainty:   

$$
H(X) = -\sum P(x_i)\log P(x_i)
$$

> Entropy measures how unpredictable or uncertain a random variable is.
> If all outcomes are equally likely, entropy is maximized.
> If one outcome is very likely (and the others are almost impossible), entropy is low.

**Joint Entropy**: Extends to multiple variables:
  
$$
  H(X, Y) = -\sum_{x,y} P(x, y)\log P(x, y)
$$

> Joint entropy measures the uncertainty of two variables together.

### 4.2 Key Insights
- Higher entropy means greater uncertainty.
- Independent variables maximize entropy.

---

## 5. ICA: The Solution

### 5.1 Problem Formulation
Given observations $Y$, ICA assumes:   

$$
Y = X\beta
$$

Where:
- You observe $Y$ (mixed signals).
- $X$: Independent sources (unknown).
- $\beta$: Mixing matrix (unknown).

>ICA decomposes observed high-dimensional data into statistically autonomous latent factors via a linear generative model, presuming non-observable source separation


### 5.2 Core Assumptions
1. Components are **statistically independent**. = No mutual information between components.
2. Components have **non-Gaussian distributions**. = Separation is mathematically impossible in gaussian sources because of the symmetry.
3. Data is **mean-centered**. = Data $Y$ must be zero mean (you subtract the mean first).

### 5.3 Solving ICA
- **Objective**: Maximize joint entropy to enforce independence.
- **Algorithm**: Gradient ascent adjusts the mixing matrix:
  
$$
W_{new} = W_{old} + h\nabla H
$$
  
  Where $h$ is the learning rate.
  and $\nabla H$ is gradient of entropy with respect to $W$

#### Step-by-Step Summary:
1. Start with the observed mixed data $Y$.
2. Preprocess it: center it (mean = 0) and sometimes whiten it (normalize variances).
3. Guess an initial mixing matrix $W$
4. Iteratively update $W$ to maximize entropy (drive outputs toward independence).
5. When updates stabilize, you have separated independent sources!

### 5.4 Practical Applications
1. **Neuroimaging**: Separate EEG or fMRI signals.
2. **Audio Processing**: Isolate voices in recordings.
3. **Finance**: Identify independent market factors.
