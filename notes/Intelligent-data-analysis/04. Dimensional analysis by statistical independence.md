# Independent Component Analysis (ICA)

## 1. Problem Statement

Independent Component Analysis (ICA) is a method for decomposing a signal into statistically independent components. Originally formulated in 1986 by Herault and Jutten, ICA addresses complex problems like **blind source separation (BSS)**, with applications ranging from audio processing to neuroimaging.

### Key Characteristics of ICA:
- **Statistical Independence**: Relaxes the orthogonality assumption in PCA and substitutes it with statistical independence.
- **Applicability**: Broad adoption due to its versatility.
- **Ill-posed Problem**: Unlike GLM or PCA, ICA requires solving for both the sources and the mixing matrix simultaneously.

#### GLM vs. ICA:
The General Linear Model (GLM) is expressed as:
```text
y = Xβ + ε
β = (XTX)^{-1}XTy
```
However, in ICA:
```text
Y = Xβ + ε
```
Here, neither β nor X is known, making the problem far more complex.

### Example: The Cocktail Party Problem
ICA can solve scenarios like separating voices in a room with multiple speakers ("cocktail party problem"). This involves identifying both the independent sources and their mixtures without prior knowledge.

---

## 2. A Brief Reminder of Key Concepts

### 2.1 Frequency and Probability Distributions
- **Frequency Table**: Enumerates the occurrences of each value.
- **Probability Distribution**: Normalizes the frequencies to probabilities.

### 2.2 Probability Density Function (PDF)
For dense variables, the PDF defines probabilities over intervals rather than specific values:
```text
f(x) = \frac{dPr(X \leq x)}{dx}
```
- **Properties**:
  - \( \int_{-\infty}^{\infty} f(x)dx = 1 \)
  - Not a specific probability (values > 1 are possible).

### 2.3 Cumulative Probability Distribution (CDF)
The CDF is defined as:
```text
F(x) = Pr(X \leq x)
```
- Properties:
  - \( F(x) \to 0 \) as \( x \to -\infty \)
  - \( F(x) \to 1 \) as \( x \to \infty \)

---

## 3. Joint Distributions and Statistical Independence

### 3.1 Joint Distribution
The joint distribution describes the probabilities of multiple variables:
```text
P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n)
```

### 3.2 Statistical Independence
Two variables are independent if:
```text
P(A \cap B) = P(A)P(B)
```
For random variables:
```text
P(X_1, X_2) = P(X_1)P(X_2)
```
**Remark**: Independence implies no mutual information but not necessarily no correlation.

---

## 4. Entropy and Information Theory

### 4.1 Entropy
Entropy quantifies uncertainty:
```text
H(X) = -\sum P(x_i)\log P(x_i)
```
- **Joint Entropy**: Extends to multiple variables:
  ```text
  H(X, Y) = -\sum_{x,y} P(x, y)\log P(x, y)
  ```

### 4.2 Key Insights
- Higher entropy means greater uncertainty.
- Independent variables maximize entropy.

---

## 5. ICA: The Solution

### 5.1 Problem Formulation
Given observations \( Y \), ICA assumes:
```text
Y = X\beta
```
Where:
- \( X \): Independent sources (unknown).
- \( \beta \): Mixing matrix (unknown).

### 5.2 Core Assumptions
1. Components are **statistically independent**.
2. Components have **non-Gaussian distributions**.
3. Data is **mean-centered**.

### 5.3 Solving ICA
- **Objective**: Maximize joint entropy to enforce independence.
- **Algorithm**: Gradient ascent adjusts the mixing matrix:
  ```text
  W_{new} = W_{old} + h\nabla H
  ```
  Where \( h \) is the learning rate.

### 5.4 Practical Applications
1. **Neuroimaging**: Separate EEG or fMRI signals.
2. **Audio Processing**: Isolate voices in recordings.
3. **Finance**: Identify independent market factors.

---

## 6. Final Remarks

- ICA solves the "cocktail party problem" by finding both the sources and the mixtures.
- Results depend heavily on the algorithm used; always document your approach.
- Basis in ICA maximizes independence, not orthogonality.
