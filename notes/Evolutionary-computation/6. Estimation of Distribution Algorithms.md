# Estimation of Distribution Algorithms

These notes introduce **Estimation of Distribution Algorithms (EDAs)**, 
a relatively new branch of evolutionary algorithms. 
Unlike traditional genetic algorithms (GAs) that rely on operators such as mutation and crossover, 
EDAs explicitly build and sample from probability models of promising candidate solutions. 
This approach aims to more directly exploit the structure of the problem.

## 1. Black-box Optimisation

**Problem Statement:**

$$
\min_{x \in \{0,1\}^n} \text{temp}(x)
$$

- **Explanation:**  
  In a black-box optimisation problem, the goal is to find a binary string (solution) that minimizes the objective function $\text{temp}(x)$. The term *black-box* indicates that the internal structure of the objective function is unknown; you only get to observe fitness values.

---

## 2. What is an Estimation of Distribution Algorithm (EDA)?

- **Definition:**  
  EDA is an evolutionary algorithm that replaces traditional operators (e.g., mutation, crossover) with two main steps:
  1. **Estimation:** Build a probability model based on the current set of promising individuals.
  2. **Sampling:** Generate new individuals by sampling from this estimated model.

- **Key Idea:**  
  Instead of applying arbitrary modifications, the algorithm “learns” which parts of the solution space are promising by explicitly modelling the distribution of good solutions.

- **Historical Reference:**  
  Refer to Pelikan, Goldberg & Cantu-Paz (2000) on linkage, distribution estimation, and Bayesian networks.

- **Advantage:**  
  By modeling the distribution, EDAs can sometimes more efficiently search the space, especially in problems where interactions between variables are critical.

---

## 3. The Basic EDA Algorithm

The overall procedure of an EDA can be broken down into the following steps:

1. **Initial Population:**  
   Generate an initial population $P_0$ of $M$ individuals uniformly at random within the search space.

2. **Loop Until Stopping Criteria is Met:**  
   For generation $l = 1, 2, \dots$:
   
   - **Selection:**  
     Select $N \leq M$ individuals from $P_{l-1}$ using a selection method based on fitness.
   
   - **Model Estimation:**  
     Estimate the probability distribution $p_l(x)$ that characterizes the selected individuals.
   
   - **Sampling:**  
     Sample $M$ new individuals from the distribution $p_l(x)$ to form the next population $P_l$.

**Clarification:**  
In each generation, the algorithm “asks”: *Given the current good solutions, what is the likelihood that a bit (or group of bits) should be 1 (or 0)?* The next generation is then formed by sampling according to these probabilities.

---

## 4. Probabilistic Models in EDAs

### 4.1. A Simple Model for Binary Strings

- **Representation:**  
  For an $n$-bit binary string, the simplest model is a **probability vector**  
  $$
  p = (p_1, p_2, \ldots, p_n)
  $$
  where each $p_i$ represents the probability that the $i$-th bit is 1.

- **Learning the Model:**  
  $p_i$ is computed as the proportion of 1's in the $i$-th position among the selected individuals.

- **Sampling New Individuals:**  
  For each bit position $i$, set:
  - $x_i = 1$ with probability $p_i$
  - $x_i = 0$ with probability $1 - p_i$

- **Note:**  
  This method treats each bit (or gene) independently. This approach is known as the **Univariate Marginal Distribution Algorithm (UMDA)**.

---

## 5. Univariate Marginal Distribution Algorithm (UMDA)

### 5.1. Model Description

- **Joint Probability:**  
  For a binary string $x = (x_1, x_2, \ldots, x_n)$, assuming independence among bits:
  
  $$
  P(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} (p_i)^{x_i} \cdot (1 - p_i)^{(1 - x_i)}
  $$
  
  This formula means that the probability of observing $x$ is the product of the probabilities for each bit.

### 5.2. The UMDA Procedure

1. **Initialization:**  
   Set the initial model as:
   
   $$
   p_0 = \left(\frac{1}{2}, \frac{1}{2}, \ldots, \frac{1}{2}\right)
   $$
   
   This represents maximum uncertainty about each bit.

2. **Sampling:**  
   At generation $t$, sample $\lambda$ individuals independently using the current probability vector $p_t$.

3. **Selection:**  
   Sort the sampled individuals in descending order according to fitness. Denote the sorted individuals as:
   
   $$
   y^{(1)}, y^{(2)}, \ldots, y^{(\lambda)}
   $$
   
4. **Model Update:**  
   Update the probability for each bit position $i$ using the top $\mu$ individuals:
   
   $$
   p_{t+1,i} = \frac{1}{\mu} \sum_{j=1}^{\mu} y^{(j)}_i
   $$
   
   Here, $y^{(j)}_i$ is the value (0 or 1) of the $i$-th bit of the $j$-th best individual.

### 5.3. UMDA with Margins

- **Problem with Extreme Values:**  
  In the update step, $p_i$ might reach 0 or 1. When this happens, the algorithm loses diversity because the corresponding bit will never change in subsequent generations.

- **Solution – Margins:**  
  Introduce lower and upper margins to keep probabilities within a safe range. The updated rule becomes:
  
  $$
  p_{t+1,i} = 
  \begin{cases}
    \frac{1}{n} & \text{if } X_i = 0, \\
    \frac{X_i}{\mu} & \text{if } 1 \leq X_i \leq \mu - 1, \\
    1 - \frac{1}{n} & \text{if } X_i = \mu,
  \end{cases}
  $$
  
  where
  
  $$
  X_i = \sum_{j=1}^{\mu} y^{(j)}_i.
  $$
  
  **Explanation:**  
  The margins (i.e., $\frac{1}{n}$ and $1 - \frac{1}{n}$) ensure that no bit's probability becomes exactly 0 or 1, thus preserving some exploration ability.

### 5.4. Example: Applying UMDA on the LeadingOnes Problem

- **LeadingOnes Function:**  
  The LeadingOnes function counts the number of consecutive ones starting from the first bit. A common mathematical formulation is:
  
  $$
  \text{LeadingOnes}(x) = \sum_{i=1}^{n} \left( i \cdot \prod_{j=1}^{i} x_j \right)
  $$
  
  This function rewards strings that have a long prefix of 1's.

- **How UMDA Works on LeadingOnes:**
  - **Step 1:** Start with $p_0 = (0.5, 0.5, \ldots, 0.5)$.
  - **Step 2:** Sample a set of individuals and evaluate their fitness using LeadingOnes.
  - **Step 3:** After selecting the top individuals, update $p_t$. For instance, if almost every selected individual starts with 1, then the updated probability $p_{t+1,1}$ becomes high (close to 1, but limited by the margin).
  - **Step 4:** Over generations, the probability vector evolves so that the first several bits increasingly become 1, which is precisely what is needed for a high LeadingOnes score.

---

## 6. Beyond UMDA: Different EDA Approaches

EDAs can differ mainly in how they model the problem:

- **Independent Variables:**  
  Models that assume each bit is independent.
  - **Examples:**  
    - Univariate Marginal Distribution Algorithm (UMDA)
    - Population Based Incremental Learning (PBIL)
    - Compact Genetic Algorithm (CGA)

- **Bivariate Dependencies:**  
  Models that capture pairwise interactions between variables.

- **Multiple Dependencies:**  
  More complex models (e.g., Bayesian networks) that capture interactions among several variables at once.

- **Discrete vs. Continuous:**  
  EDAs are also adapted to continuous domains by modeling continuous probability distributions.

**Clarification:**  
The more complex the model (from univariate to multivariate), the better it can capture interdependencies in the problem. However, complex models often require more computational effort and more data to learn accurately.

---

## 7. Comparison with Traditional Genetic Algorithms (GAs)

- **Traditional GA:**
  - Uses crossover and mutation operators to generate new individuals.
  - The operators are typically applied without a direct model of the problem structure.

- **EDA:**
  - Replaces these operators with a process of building and sampling from an estimated probability distribution.
  - The model explicitly represents what good solutions look like.

- **Advantage of EDAs:**  
  If the probabilistic model is a good representation of the problem, EDAs can generate higher-quality solutions by directly exploiting the learned structure.

---

## 8. Example Problems Addressed by EDAs

### 8.1. Example Problem 2: Subset Sum

- **Problem Statement:**  
  Given a set of integers and a target weight $W$, find a subset of the integers such that the sum equals $W$.

- **Example:**  
  For the set $\{1, 3, 5, 6, 8, 10\}$ and $W = 14$, possible solutions include:
  - $\{1, 3, 10\}$
  - $\{3, 5, 6\}$
  - $\{6, 8\}$
  - $\{1, 5, 8\}$

- **EDA Approach:**  
  Represent each number’s inclusion (1) or exclusion (0) as bits in a binary string. Then, use an EDA to evolve a population of such strings that likely add up to the target sum.

### 8.2. Example Problem 3: Concatenated Traps

- **Problem Setup:**  
  The solution is a binary string divided into groups (e.g., groups of 5 bits). Each group contributes to the fitness via a trap function.

- **Trap Function Characteristics:**
  - Designed so that while the global optimum is the string of all 1’s, local statistics can be misleading.
  - **Observation:**  
    For a single trap, the average fitness of a string starting with “0****” might be higher than that starting with “1****”. For instance, the average fitness of “0****” might be 2, while that of “1****” might be 1.375.

- **Why the Simple Probability Vector Fails:**
  - Modeling each bit independently (using a univariate model) fails to capture the interaction among bits within a group.
  - **Result:**  
    The algorithm is misled because the marginal (individual bit) probabilities do not accurately reflect the group’s overall contribution to fitness.

- **Solution – Group-Based Modeling:**
  - Instead of using one-bit statistics, consider the statistics of the entire 5-bit group.
  - **Approach:**  
    Learn the probability distribution over all possible 5-bit strings (e.g., compute $p(00000), p(00001), \dots, p(11111)$).
  - **Advantage:**  
    This approach captures the dependencies among bits and correctly identifies that the group “11111” is the best choice.

---

## 9. Advanced Topics

### 9.1. Bayesian Optimization Algorithm (BOA)

- **Description:**  
  BOA uses Bayesian networks to capture complex dependencies among variables.
- **Research Focus:**  
  Much research is dedicated to learning effective dependency models. Bayesian networks, for example, can capture both simple and intricate interactions in the solution space.
- **Advantage:**  
  A well-learned Bayesian network can significantly improve the search for optimal solutions in complex problems.

### 9.2. Continuous-Valued EDAs

- **Scope:**  
  These algorithms extend EDA principles to continuous optimization problems.
- **Applications:**  
  - Two-dimensional (2D) optimization problems.
  - Multi-modal problems where several local optima exist.
- **Visual Insights:**  
  Graphical representations (plots) are often used to illustrate how continuous EDAs navigate complex landscapes.

---

## 10. Conclusions

- **EDA Overview:**  
  EDAs form a new class of evolutionary algorithms that do not rely on conventional crossover and mutation operators. Instead, they estimate a probability model of the selected (good) individuals and generate new offspring by sampling from this model.
  
- **Benefits:**
  - Can outperform conventional evolutionary algorithms if the probabilistic model accurately reflects the underlying problem.
  - Provide a series of evolving models that offer insight into the problem structure.
  
- **Challenges:**
  - Estimating the correct model is non-trivial, especially when the problem structure is unknown.
  - More complex models that capture variable dependencies require additional computational resources and data.

---

## Additional Clarifications

- **Black-box Optimization Context:**  
  EDAs are particularly useful in scenarios where the objective function is opaque and traditional gradient-based methods cannot be applied.

- **Choosing a Model:**  
  A simple univariate model (like UMDA) may work for problems where variables act independently. For problems with strong interactions (e.g., concatenated traps), more sophisticated models that capture dependencies are necessary.

- **Margins in Model Update:**  
  Implementing margins helps maintain diversity in the population by preventing probabilities from becoming too extreme too soon.

This resource is intended to serve as a comprehensive guide that both reflects the original lecture notes and offers additional insights and examples to facilitate deeper understanding.
